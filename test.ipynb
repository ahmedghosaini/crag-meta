{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4483a7c7c24909a30d9e0dc6aff715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Predictions: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You will be provided with additional information between the \"<DOCS>\" tags. Keep your answers brief, ideally less than 20 words, but a strict limit of 30 words.\n",
      "\n",
      "If the provided information is insufficient for answering the question, simply output \"Insuffient information\" and only output that. If the question asserts a false premise, like \"When did Eisenhower become Prime Minister?\", simply output \"Invalid question\". Thus, if you are inclined to say something like \"X never has\" or \"X never was\" or \"X never did\", your output should be \"Invalid question\".\n",
      "\n",
      "Final reminder, with award shows, years can be tricky. Often awards are handed out the year after the project is made. Thus for the Oscars in 2015, the awards are being given out to movies made in 2014.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "# Example 1\n",
      "QUESTION: who was the first nba player to get a recorded triple double in basketball?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Andy Phillip got a triple-double versus the Fort Wayne Pistons on December 14, 1950.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: andy phillip\n",
      "\n",
      "# Example 2\n",
      "QUESTION: what year did ronald reagan become kommissar in the soviet union?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Ronald Wilson Reagan was an American politician and actor who served as the 40th president of the United States from 1981 to 1989.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Reagan's policies also helped contribute to the end of the Cold War and the end of Soviet communism.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: invalid question\n",
      "\n",
      "# Example 3\n",
      "QUESTION: what are the total number of farms in nebraska?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "There are many farms in Nebraska, some growing corn, while other growing sugar beats.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Nebraska's largest industries are the argiculture and cattle production industries.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: i don't know\n",
      "\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Microsoft Office 2019 - Wikipedia\n",
      "Microsoft Office 2019 (second release codenamed Office 16) is a version of\n",
      "Microsoft Office for both Windows and Mac. It replaces Office 2016 and was\n",
      "replaced by Office 2021 on October 5, 2021.[8] It was unveiled on April 27,\n",
      "2018, for Windows 10 and June 12, 2018, for macOS, and launched on September\n",
      "24, 2018.[1] Some features that had previously been restricted to Office 365\n",
      "subscribers are available in this release.[9] Office 2019 retains the same\n",
      "major version number of 16 that Office 2016 had, making it the second\n",
      "perpetual release of Office 16. Microsoft ended mainstream support for Office\n",
      "2019 on October 10, 2023. Unlike other versions of Microsoft Office, Office\n",
      "2019 will only get two years of extended support, which means that support for\n",
      "Office 2019 will end on the same day as support for Office 2016 and Windows\n",
      "10, on October 14, 2025.[10]\n",
      "</DOC>\n",
      "<DOC>\n",
      "Office 2019 vs. Office 2016, Q&A: Licensing, Requirements, Benchmark, ...\n",
      "ru_project_professional_2019_x86_x64_dvd_d8a4bf9b.iso)\n",
      "For as long as only one language of Office is installed, it is possible to\n",
      "setup additional Office applications of the same language, regardless of which\n",
      "language is it.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Microsoft Office 2019 - Wikipedia\n",
      "## Editions[edit]\n",
      "### Traditional editions[edit]\n",
      "Like its predecessor Microsoft Office 2016, Microsoft Office 2019 has the same\n",
      "perpetual SKU editions aimed towards different markets. Like its predecessor,\n",
      "Microsoft Office 2019 contains Word, Excel, PowerPoint and OneNote and is\n",
      "licensed for use on one computer.[18][19]\n",
      "</DOC>\n",
      "<DOC>\n",
      "Microsoft Office 2019 - Wikipedia\n",
      "Toggle the table of contents\n",
      "\n",
      "# Microsoft Office 2019\n",
      "\n",
      "21 languages\n",
      "\n",
      "  * العربية\n",
      "  * বাংলা\n",
      "  * Català\n",
      "  * Čeština\n",
      "  * Deutsch\n",
      "  * Ελληνικά\n",
      "  * Español\n",
      "  * فارسی\n",
      "  * Français\n",
      "  * 한국어\n",
      "  * Bahasa Indonesia\n",
      "  * Italiano\n",
      "  * עברית\n",
      "  * Polski\n",
      "  * Português\n",
      "  * Русский\n",
      "  * Simple English\n",
      "  * Türkçe\n",
      "  * Українська\n",
      "  * Tiếng Việt\n",
      "  * 中文\n",
      "\n",
      "Edit links\n",
      "</DOC>\n",
      "<DOC>\n",
      "Microsoft Office 2019 - Wikipedia\n",
      "* ^ \"What languages is Office available in?\". Microsoft. Archived from the original on April 7, 2019. Retrieved October 11, 2023.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Microsoft Office 2019 - Wikipedia\n",
      "For Office 2013 and 2016, various editions containing the client apps were\n",
      "available in both Click-To-Run (inspired by Microsoft App-V) and traditional\n",
      "Windows Installer setup formats. However, Office 2019 client apps only have a\n",
      "Click-to-Run installer and only the server apps have the traditional MSI\n",
      "installer. The Click-To-Run version has a smaller footprint; in case of\n",
      "Microsoft Office 2019 Pro Plus, the product requires 10 GB less than the MSI\n",
      "version of Office 2016 Pro Plus.[22]\n",
      "</DOC>\n",
      "<DOC>\n",
      "Microsoft Office 2019 - Wikipedia\n",
      "Type| Office suite\n",
      "License| Trialware, software as a service\n",
      "Website| office.com\n",
      "Microsoft Office 2019 for MacDeveloper(s)| Microsoft  \n",
      "---|---  \n",
      "Initial release| September 24, 2018; 5 years ago (2018-09-24)  \n",
      "Operating system| macOS Sierra or later[6]  \n",
      "Platform| x64  \n",
      "Predecessor| Microsoft Office 2016  \n",
      "Successor| Microsoft Office 2021  \n",
      "Available in| 27 languages[7]  \n",
      "List of languagesEnglish, Arabic, Chinese (Simplified), Chinese (Traditional),\n",
      "Czech, Danish, Dutch, Finnish, French, German, Greek, Hebrew, Hungarian,\n",
      "Indonesian, Italian, Japanese, Korean, Norwegian (Bokmål), Polish, Portuguese\n",
      "(Portugal), Portuguese (Brazil), Russian, Slovak, Spanish, Swedish, Thai,\n",
      "Turkish\n",
      "</DOC>\n",
      "<DOC>\n",
      "Office 2019 vs. Office 2016, Q&A: Licensing, Requirements, Benchmark, ...\n",
      "Installing Office in Russian\n",
      "## Where Can Additional Office Languages Be Found?\n",
      "Additional languages can be downloaded directly from Microsoft for free from\n",
      "the following link. Currently Office 2010, 2013, 2016 and 2019 are supported:\n",
      "https://go.microsoft.com/fwlink/?LinkId=614981\n",
      "</DOC>\n",
      "<DOC>\n",
      "Office 2019 vs. Office 2016, Q&A: Licensing, Requirements, Benchmark, ...\n",
      "Downloading Additional Languages for Office Directly from Microsoft, for Free\n",
      "## Are Office 2016 & Office 2019 the same? What’s new in Office 2019?\n",
      "The licensing agreement is more limiting in Office 2019, and it is slower\n",
      "compared with Office 2016 and 2013. Office 2019 also requires Windows 10 1809,\n",
      "and cannot run on Windows 7, for example.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Microsoft Office 2019 - Wikipedia\n",
      "Platform| IA-32, x64, ARM, Web\n",
      "Predecessor| Microsoft Office 2016 (2015)\n",
      "Successor| Microsoft Office 2021 (2021)\n",
      "Available in| 102 languages[5]\n",
      "List of languages\n",
      "\n",
      "  * Full (40): English, Arabic, Bulgarian, Chinese (Simplified), Chinese (Traditional), Croatian, Czech, Danish, Dutch, Estonian, Finnish, French, German, Greek, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Kazakh, Korean, Latvian, Lithuanian, Malay (Latin), Norwegian Bokmål, Polish, Portuguese (Brazil), Portuguese (Portugal), Romanian, Russian, Serbian (Latin, Serbia), Slovak, Slovenian, Spanish, Swedish, Thai, Turkish, Ukrainian, Vietnamese\n",
      "  * Partial (51): Afrikaans, Albanian, Amharic, Armenian, Assamese, Azerbaijani (Latin), Bangla (Bangladesh), Bangla (Bengali India), Basque (Basque), Belarusian, Bosnian (Latin), Catalan, Dari, Filipino, Galician, Georgian, Gujarati, Icelandic, Irish, Kannada, Khmer, KiSwahili, Konkani, Kyrgyz, Luxembourgish, Macedonian (Republic of Macedonia), Malayalam, Maltese, Maori, Marathi, Mongolian (Cyrillic), Nepali, Norwegian Nynorsk, Odia, Persian (Farsi), Punjabi (Gurmukhi), Quechua, Scottish Gaelic, Serbian (Cyrillic, Bosnia & Herzegovina), Serbian (Cyrillic, Serbia), Sindhi (Arabic), Sinhala, Tamil, Tatar (Cyrillic), Telugu, Turkmen (Latin), Urdu, Uyghur, Uzbek (Latin), Valencian, Welsh\n",
      "  * Proofing only (11): Hausa, Igbo, isiXhosa, isiZulu, Kinyarwanda, Pashto, Romansh, Sesotho sa Leboa, Setswana, Wolof, Yoruba\n",
      "</DOC>\n",
      "</DOCS>\n",
      "QUESTION (asked at 02/28/2024, 10:04:54 PT): is microsoft office 2019 available in a greater number of languages than microsoft office 2013?<|eot_id|><|start_header_id|>assistant<|end_header_id|>ANSWERABLE:\n",
      "############ Query: is microsoft office 2019 available in a greater number of languages than microsoft office 2013?\n",
      "############ answer: yes\n",
      "############\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You will be provided with additional information between the \"<DOCS>\" tags. Keep your answers brief, ideally less than 20 words, but a strict limit of 30 words.\n",
      "\n",
      "If the provided information is insufficient for answering the question, simply output \"Insuffient information\" and only output that. If the question asserts a false premise, like \"When did Eisenhower become Prime Minister?\", simply output \"Invalid question\". Thus, if you are inclined to say something like \"X never has\" or \"X never was\" or \"X never did\", your output should be \"Invalid question\".\n",
      "\n",
      "Final reminder, with award shows, years can be tricky. Often awards are handed out the year after the project is made. Thus for the Oscars in 2015, the awards are being given out to movies made in 2014.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "# Example 1\n",
      "QUESTION: who was the first nba player to get a recorded triple double in basketball?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Andy Phillip got a triple-double versus the Fort Wayne Pistons on December 14, 1950.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: andy phillip\n",
      "\n",
      "# Example 2\n",
      "QUESTION: what year did ronald reagan become kommissar in the soviet union?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Ronald Wilson Reagan was an American politician and actor who served as the 40th president of the United States from 1981 to 1989.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Reagan's policies also helped contribute to the end of the Cold War and the end of Soviet communism.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: invalid question\n",
      "\n",
      "# Example 3\n",
      "QUESTION: what are the total number of farms in nebraska?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "There are many farms in Nebraska, some growing corn, while other growing sugar beats.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Nebraska's largest industries are the argiculture and cattle production industries.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: i don't know\n",
      "\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Dow 30 Stocks List Today • Dogs of the Dow\n",
      "## Which Dow stock pays the highest dividend?\n",
      "\n",
      "Of the 30 Dow Jones Industrial Companies, IBM pays the highest dividend with\n",
      "an annual dividend yield of over 5%.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Best Dow Jones Stocks To Watch In 2024\n",
      "The Dow Jones Industrial Average ended February at new highs, as the ongoing\n",
      "stock market rally continues. The best Dow Jones stocks to watch in 2024 are\n",
      "Apple (AAPL), Merck (MRK), Microsoft (MSFT) and Visa (V).\n",
      "</DOC>\n",
      "<DOC>\n",
      "Best Dow Jones Stocks To Watch In 2024\n",
      "\n",
      "* Research\n",
      "\n",
      "#  Today's Dow Jones Stocks To Watch: Apple Stock Dives On Slowing iPhone\n",
      "Sales\n",
      "\n",
      "Licensing\n",
      "\n",
      "  * SCOTT LEHTONEN\n",
      "  * 02:20 PM ET 03/05/2024\n",
      "</DOC>\n",
      "<DOC>\n",
      "Best Dow Jones Stocks To Watch In 2024\n",
      "## IBM Stock\n",
      "IBM shares are approaching a flat base's 196.90 buy point.\n",
      "The stock dipped 0.2% Tuesday.\n",
      "## Merck Stock\n",
      "Drug giant Merck is breaking out past a 119.65 cup-base buy point.\n",
      "Merck shares fell 1% Tuesday, back in the 5% buy area above the entry and\n",
      "pulling back to the 50-day line.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Best Dow Jones Stocks To Watch In 2024\n",
      "The worst three Dow Jones stocks in 2023 were Walgreens Boots Alliance (WBA),\n",
      "Chevron (CVX) and Johnson & Johnson (JNJ), with respective declines of 30, 16%\n",
      "and 11%.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Best Dow Jones Stocks To Watch In 2024\n",
      "On the downside, Intel (INTC) and Salesforce (CRM) were the biggest decliners,\n",
      "both sliding 5%.\n",
      "Amid the current stock market rally — according to the IBD Big Picture —\n",
      "investors should focus on stocks that show strong relative strength. These\n",
      "could again become 2024 market leaders if the Dow Jones Industrial Average is\n",
      "able to extend its recent gains.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Best Dow Jones Stocks To Watch In 2024\n",
      "The best Dow Jones stocks to buy and watch in 2024 include Apple, IBM, Merck,\n",
      "Microsoft and Visa.\n",
      "Stock Market ETF Strategy And How To Invest\n",
      "## Dow Jones Leader: Apple Stock\n",
      "Among Dow Jones stocks in the Magnificent Seven, Apple stock lost 2.9% Tuesday\n",
      "on a report that showed slowing iPhone sales in China. The company sold 24%\n",
      "fewer iPhones in China in the first six weeks of 2024 than a year earlier,\n",
      "according to research firm Counterpoint Research.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Best Dow Jones Stocks To Watch In 2024\n",
      "###### Source: IBD Data As Of Dec. 29, 2023\n",
      "## Today's Dow Jones Movers\n",
      "Inside the Dow Jones Industrial Average, Walmart (WMT) was one of Tuesday's\n",
      "biggest gainers, up 1.6%. Shares are extended past a cup-with-handle entry at\n",
      "54.47.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Best Dow Jones Stocks To Watch In 2024\n",
      "X\n",
      "There are clear winners — and losers — at the start of February. The top three\n",
      "performing blue chip stocks in 2023 were Salesforce (CRM), Intel (INTC) and\n",
      "Microsoft, posting rallies of 98%, 90% and 56%, respectively.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Top 30 Companies of Dow Jones Index by Weight in 2023\n",
      "## Best Dow Jones ETF\n",
      "Investing in an ETF (Exchange-Traded Fund) is another way to invest in the Dow\n",
      "Jones index. The best Dow Jones ETF in the market today is SPDR Dow Jones\n",
      "Industrial Average ETF (DIA). It is a non-leveraged ETF that copies the\n",
      "performance of the Dow and its stock allocation.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "QUESTION (asked at 03/05/2024, 23:18:31 PT): what company in the dow jones is the best performer today?<|eot_id|><|start_header_id|>assistant<|end_header_id|>ANSWERABLE:\n",
      "############ Query: what company in the dow jones is the best performer today?\n",
      "############ answer: walmart\n",
      "############\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You will be provided with additional information between the \"<DOCS>\" tags. Keep your answers brief, ideally less than 20 words, but a strict limit of 30 words.\n",
      "\n",
      "If the provided information is insufficient for answering the question, simply output \"Insuffient information\" and only output that. If the question asserts a false premise, like \"When did Eisenhower become Prime Minister?\", simply output \"Invalid question\". Thus, if you are inclined to say something like \"X never has\" or \"X never was\" or \"X never did\", your output should be \"Invalid question\".\n",
      "\n",
      "Final reminder, with award shows, years can be tricky. Often awards are handed out the year after the project is made. Thus for the Oscars in 2015, the awards are being given out to movies made in 2014.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "# Example 1\n",
      "QUESTION: who was the first nba player to get a recorded triple double in basketball?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Andy Phillip got a triple-double versus the Fort Wayne Pistons on December 14, 1950.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: andy phillip\n",
      "\n",
      "# Example 2\n",
      "QUESTION: what year did ronald reagan become kommissar in the soviet union?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Ronald Wilson Reagan was an American politician and actor who served as the 40th president of the United States from 1981 to 1989.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Reagan's policies also helped contribute to the end of the Cold War and the end of Soviet communism.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: invalid question\n",
      "\n",
      "# Example 3\n",
      "QUESTION: what are the total number of farms in nebraska?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "There are many farms in Nebraska, some growing corn, while other growing sugar beats.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Nebraska's largest industries are the argiculture and cattle production industries.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: i don't know\n",
      "\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Shine like a diamond: 7 most successful celebrity beauty brands ...\n",
      "###\n",
      "\n",
      "Feud or no feud? Selena Gomez and Kylie Jenner’s top twinning fashion moments\n",
      "### 4\\. Ariana Grande: R.E.M.\n",
      "Singer Ariana Grande launched her vegan make-up line R.E.M. in 2021. Photo:\n",
      "@r.e.m.beauty/Instagram\n",
      "In 2021, musician Ariana Grande expanded her reach with the launch of her\n",
      "vegan make-up line, R.E.M. The first collection included eyeliners, false\n",
      "lashes, highlighters and plumping gloss. R.E.M. was being sold in mega beauty\n",
      "store Ulta by April 2022, reports Seventeen. But the brand has suffered\n",
      "hiccups after the company Grande partnered with filed for bankruptcy.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Shine like a diamond: 7 most successful celebrity beauty brands ...\n",
      "Gwyneth Paltrow owns a 30 per cent stake in Goop. Photo: Goop\n",
      "Paltrow regularly promotes her brand with skincare ideas and tips on how she\n",
      "gets her flawless skin, which end up going viral. Cosmopolitan reports the\n",
      "Iron Man actress owns a 30 per cent stake in Goop, which is worth an estimated\n",
      "US$250 million.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Shine like a diamond: 7 most successful celebrity beauty brands ...\n",
      "###\n",
      "\n",
      "Inside the luxury lives of the Spencer sisters – Amelia, Eliza and Kitty\n",
      "### 2\\. Kylie Jenner: Kylie Cosmetics\n",
      "Kylie Jenner showing off Kylie Cosmetics’ new Glow Balm and Gloss Drip. Photo:\n",
      "@kyliejenner/Instagram\n",
      "</DOC>\n",
      "<DOC>\n",
      "These Celebs Have Turned Their Love Of Makeup And Skincare Into ...\n",
      "\n",
      "1. Beauty\n",
      "  2. Skin\n",
      "  3. 40 Celebrities Who Started Their Own Successful Beauty Brands\n",
      "\n",
      "# 40 Celebrities Who Started Their Own Successful Beauty Brands\n",
      "\n",
      "Talk about a side hustle.\n",
      "\n",
      "By Georgia DavisPublished: May 6, 2020\n",
      "\n",
      "Save Article\n",
      "</DOC>\n",
      "<DOC>\n",
      "Shine like a diamond: 7 most successful celebrity beauty brands ...\n",
      "Advertisement\n",
      "Advertisement\n",
      "Advertisement\n",
      "Celebrities\n",
      "## 7 most successful celebrity beauty brands in the world, from Selena Gomez’\n",
      "Rare Beauty and Rihanna’s multimillion-dollar Fenty Beauty, to Kylie Jenner’s\n",
      "Kylie Cosmetics and Gwyneth Paltrow’s Goop\n",
      "</DOC>\n",
      "<DOC>\n",
      "Successful Celebrity Beauty Brands | Cosmetify\n",
      "### Rare Beauty\n",
      "Completing this top three list of the most successful celebrity beauty brands,\n",
      "with a score of 8.97 /10, is Selena Gomez’s brand, Rare Beauty. With a mission\n",
      "statement to ‘break down unrealistic standards of perfection’, Rare Beauty\n",
      "sets itself apart from other brands in the industry by placing a focus on the\n",
      "importance of mental health alongside the sale of their high-quality make-up\n",
      "products. As the only entry not to have been featured in our previous edition\n",
      "of the top three, Rare Beauty is arguably the most impressive on this list!\n",
      "</DOC>\n",
      "<DOC>\n",
      "Celebrity Beauty Brands Aren’t Guaranteed Money Makers Anymore\n",
      "In 2020, more than a third of Morphe Cosmetics revenue came from brands\n",
      "associated with celebrities and famous YouTube beauty influencers. The\n",
      "eyeshadow retailer severed ties with James Charles and Jeffree Star due to\n",
      "inappropriate online conduct. Attempts to shore up the bottom line with\n",
      "Grande’s r.e.m. beauty and other influencer collaborations ultimately proved\n",
      "futile when Forma filed for Chapter 11 bankruptcy on January 12.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Shine like a diamond: 7 most successful celebrity beauty brands ...\n",
      "The company Ariana Grande partnered with filed for bankruptcy earlier this\n",
      "year and the future of R.E.M. remains unclear. Photo: @r.e.m.beauty/Instagram\n",
      "</DOC>\n",
      "<DOC>\n",
      "Shine like a diamond: 7 most successful celebrity beauty brands ...\n",
      "### 3\\. Selena Gomez: Rare Beauty\n",
      "Selena Gomez started her own beauty brand Rare Beauty. Photo: Rare Beauty\n",
      "The power of the singer’s beauty brand focuses on “breaking down unrealistic\n",
      "standards of perfection”. Gomez has also highlighted the importance of mental\n",
      "health through her range, using uplifting statements on her products.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Shine like a diamond: 7 most successful celebrity beauty brands ...\n",
      "STORYLynn Farah\n",
      "\n",
      "Apr 13 , 2023\n",
      "Celebrities that have their own successful beauty brands, from Selena Gomez\n",
      "and Rihanna to Gwyneth Paltrow. Photos: @rarebeauty, @fentybeauty,\n",
      "@goop/Instagram\n",
      "</DOC>\n",
      "</DOCS>\n",
      "QUESTION (asked at 03/13/2024, 08:29:31 PT): name three celebrities who have been involved in successful collaborations with beauty brands.<|eot_id|><|start_header_id|>assistant<|end_header_id|>ANSWERABLE:\n",
      "############ Query: name three celebrities who have been involved in successful collaborations with beauty brands.\n",
      "############ answer: Ariana Grande, Gwyneth Paltrow, Kylie Jenner\n",
      "############\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You will be provided with additional information between the \"<DOCS>\" tags. Keep your answers brief, ideally less than 20 words, but a strict limit of 30 words.\n",
      "\n",
      "If the provided information is insufficient for answering the question, simply output \"Insuffient information\" and only output that. If the question asserts a false premise, like \"When did Eisenhower become Prime Minister?\", simply output \"Invalid question\". Thus, if you are inclined to say something like \"X never has\" or \"X never was\" or \"X never did\", your output should be \"Invalid question\".\n",
      "\n",
      "Final reminder, with award shows, years can be tricky. Often awards are handed out the year after the project is made. Thus for the Oscars in 2015, the awards are being given out to movies made in 2014.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "# Example 1\n",
      "QUESTION: who was the first nba player to get a recorded triple double in basketball?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Andy Phillip got a triple-double versus the Fort Wayne Pistons on December 14, 1950.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: andy phillip\n",
      "\n",
      "# Example 2\n",
      "QUESTION: what year did ronald reagan become kommissar in the soviet union?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Ronald Wilson Reagan was an American politician and actor who served as the 40th president of the United States from 1981 to 1989.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Reagan's policies also helped contribute to the end of the Cold War and the end of Soviet communism.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: invalid question\n",
      "\n",
      "# Example 3\n",
      "QUESTION: what are the total number of farms in nebraska?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "There are many farms in Nebraska, some growing corn, while other growing sugar beats.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Nebraska's largest industries are the argiculture and cattle production industries.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: i don't know\n",
      "\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Heaven vs Hell - Difference and Comparison | Diffen\n",
      "#### Hell\n",
      "Muslims believe in jahannam (in Arabic: جهنم) (which comes from the Hebrew\n",
      "word gehennim and resembles the versions of Hell in Christianity). In the\n",
      "Qur'an, the holy book of Islam, there are literal descriptions of the\n",
      "condemned in a fiery Hell, as contrasted to the garden-like Paradise (jannah)\n",
      "enjoyed by righteous believers. In addition, Heaven and Hell are split into\n",
      "many different levels depending on the actions perpetrated in life, where\n",
      "punishment is given depending on the level of evil done in life, and good is\n",
      "separated into other levels depending on how well one followed God while\n",
      "alive. There is an equal number of mentions of both Hell and paradise in the\n",
      "Qur'an, which is considered by believers to be among the numeric miracles in\n",
      "the Qur'an.[citation needed] The Islamic concept of Hell is similar to the\n",
      "medieval Christian view of Dante. [citation needed] However, Satan is not\n",
      "viewed as Hell's ruler, merely one of its sufferers. The gate of Hell is\n",
      "guarded by Maalik also known as Zabaaniyah. The Quran states that the fuel of\n",
      "Hellfire is rocks/stones (idols) and human beings. Names of Hell according to\n",
      "Islamic Tradition based on the Quranic ayah and Hadith:\n",
      "</DOC>\n",
      "<DOC>\n",
      "Heaven and Hell | Cru\n",
      "As the template of continuity/discontinuity can keep us from\n",
      "overspiritualizing heaven, so we also want to be aware that Scripture uses\n",
      "symbolic language to describe the new heavens and new earth – language meant\n",
      "to convey concepts, not actual geography. Symbolism is shorthand for reality.\n",
      "When you see a symbol for a women’s room, you know what it means, but women\n",
      "don’t look like that graphic, do they?\n",
      "</DOC>\n",
      "<DOC>\n",
      "Heaven vs Hell - Difference and Comparison | Diffen\n",
      "#### Hell\n",
      "In Christianity, the popularly used word Hell, however, is a translation of\n",
      "three Greek words: hades, Gehenna, and Tartarus. Hades, literally meaning\n",
      "unseen, usually refers to the state of death, which is defined by some as a\n",
      "conscious waiting place for resurrection, and by others as a state of\n",
      "unconsciousness synonymous with death itself. Gehenna, on the other hand, more\n",
      "ambiguous than hades, seems to refer to judgment and fits more closely with\n",
      "the modern conceptions of Hell. Tartarus is used in reference to the judgment\n",
      "of sinning angels and seems to be an allusion to Greek mythology (see\n",
      "Tartarus). While the majority of Christianity views Hell as a place of eternal\n",
      "torment, some Christians, such as Universalist Christians (see Universalism)\n",
      "contend that after resurrection, unrepentant sinners are judged and purified\n",
      "in the lake of fire and then later accepted into Heaven, while others believe\n",
      "that after resurrection, the unrepentant sinners are permanently destroyed in\n",
      "the lake of fire (see annihilationism). Various interpretations of the\n",
      "torments of Hell exist, ranging from fiery pits of wailing sinners to lonely\n",
      "isolation from God's presence. However, the descriptions of Hell found in the\n",
      "Bible are quite vague. The books of Matthew, Mark, and Jude tell of a place of\n",
      "fire, while the books of Luke and Revelation report it as an abyss. Our\n",
      "modern, more graphic, images of Hell have developed from writings that are not\n",
      "found in the Bible. Dante's The Divine Comedy is a classic inspiration for\n",
      "modern images of Hell. Other early Christian writings also illustrate the\n",
      "anguish of Hell. Most Christians believe that damnation occurs immediately\n",
      "upon death (particular judgment), and others that it occurs after Judgment\n",
      "Day, which is written about in the book of Revelation.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Heaven vs Hell - Difference and Comparison | Diffen\n",
      "endtoc\n",
      "## Definition\n",
      "### Heaven\n",
      "Originally the term \"heaven\" referred to the sky or the area above the earth\n",
      "where the \"heavenly bodies\" are placed. This is the main meaning of the word\n",
      "in the Bible. It was considered the dwelling place of God and his angels.\n",
      "However, with time, the term came to be used also in the sense of the abode of\n",
      "the righteous at some point after death. This is supported by a few verses in\n",
      "the Bible, but the Bible tends to use other terms, such as Paradise, for this.\n",
      "(See below for other terms.)\n",
      "</DOC>\n",
      "</DOCS>\n",
      "QUESTION (asked at 03/19/2024, 23:28:32 PT): what is the other language between heaven and hell came out in originally?<|eot_id|><|start_header_id|>assistant<|end_header_id|>ANSWERABLE:\n",
      "############ Query: what is the other language between heaven and hell came out in originally?\n",
      "############ answer: The concepts of Heaven and Hell originate from religious texts, primarily in Hebrew and Arabic for Judaism and Islam respectively, and in Greek for Christianity.\n",
      "############\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You will be provided with additional information between the \"<DOCS>\" tags. Keep your answers brief, ideally less than 20 words, but a strict limit of 30 words.\n",
      "\n",
      "If the provided information is insufficient for answering the question, simply output \"Insuffient information\" and only output that. If the question asserts a false premise, like \"When did Eisenhower become Prime Minister?\", simply output \"Invalid question\". Thus, if you are inclined to say something like \"X never has\" or \"X never was\" or \"X never did\", your output should be \"Invalid question\".\n",
      "\n",
      "Final reminder, with award shows, years can be tricky. Often awards are handed out the year after the project is made. Thus for the Oscars in 2015, the awards are being given out to movies made in 2014.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "# Example 1\n",
      "QUESTION: who was the first nba player to get a recorded triple double in basketball?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Andy Phillip got a triple-double versus the Fort Wayne Pistons on December 14, 1950.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: andy phillip\n",
      "\n",
      "# Example 2\n",
      "QUESTION: what year did ronald reagan become kommissar in the soviet union?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Ronald Wilson Reagan was an American politician and actor who served as the 40th president of the United States from 1981 to 1989.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Reagan's policies also helped contribute to the end of the Cold War and the end of Soviet communism.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: invalid question\n",
      "\n",
      "# Example 3\n",
      "QUESTION: what are the total number of farms in nebraska?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "There are many farms in Nebraska, some growing corn, while other growing sugar beats.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Nebraska's largest industries are the argiculture and cattle production industries.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: i don't know\n",
      "\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Rory McIlroy's majors timeline: Breaking down golfer's last major ...\n",
      "U.S. OPEN 2022: Why Justin Thomas, Rory McIlroy are among best bets to win\n",
      "## Has Rory McIlroy won the U.S. Open?\n",
      "McIlroy has won the U.S. Open. In fact, it was the first major he won during\n",
      "his career.\n",
      "McIlroy, then 22, won the U.S. Open in 2011 at Congressional Golf Club in\n",
      "Bethesda, Md. He ran away with the event, carding a 16-under for the\n",
      "tournament and winning by eight strokes. He led the tournament from wire to\n",
      "wire thanks to the 6-under 65 he posted in Round 1 of the tournament.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Rory McIlroy's majors timeline: Breaking down golfer's last major ...\n",
      "McIlroy has been in contention at plenty of major tournaments, but he hasn't\n",
      "won one since 2014. He is looking to change that at the 2023 U.S. Open and\n",
      "enters the final day of the tournament one stroke off the lad.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Career | Rory McIlroy\n",
      "##  2023\n",
      "\n",
      "### Rory wins the DP World Tour title for a fifth time.\n",
      "\n",
      "Explore\n",
      "\n",
      "Close\n",
      "\n",
      "Rory wins the DP World Tour title for a fifth time.\n",
      "\n",
      "* * *\n",
      "\n",
      "I think winning shows my consistency year-to-year. Over the last 10 years, I\n",
      "think I've won eight season-long titles between America and between here, so\n",
      "it just shows my level of consistency.\n",
      "\n",
      "* * *\n",
      "\n",
      "Rory McIlroy\n",
      "</DOC>\n",
      "<DOC>\n",
      "Rory McIlroy's majors timeline: Breaking down golfer's last major ...\n",
      "That campaign is rivaled only by his 2022 major appearances. He finished\n",
      "top-10 in each of the four tournaments finishing second at the Masters, eighth\n",
      "at the PGA Championship, tied for fifth at the U.S. Open and third at The Open\n",
      "Championship.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Career | Rory McIlroy\n",
      "HTML\n",
      "\n",
      "\n",
      "#\n",
      "\n",
      "  * Rory\n",
      "  * Career\n",
      "  * Schedule\n",
      "  * Partners\n",
      "  * Rory Foundation\n",
      "AJAX\n",
      "# Career\n",
      "## 2024\n",
      "##  2024\n",
      "\n",
      "### Rory wins his fourth Hero Dubai Desert Classic.\n",
      "\n",
      "Explore\n",
      "\n",
      "Close\n",
      "\n",
      "Rory wins the tournament for the 4th time.\n",
      "\n",
      "* * *\n",
      "\n",
      "I’ve never lost the hunger to go out and play better, but I’ve also never lost\n",
      "the joy of the game.\n",
      "\n",
      "* * *\n",
      "\n",
      "Rory McIlroy\n",
      "</DOC>\n",
      "<DOC>\n",
      "Rory McIlroy's majors timeline: Breaking down golfer's last major ...\n",
      "McIlroy won his first major in 2011 at the U.S. Open. He ran away with the\n",
      "tournament, winning by eight strokes. He and Robert Garrigus became the fifth\n",
      "and sixth players to shoot under par in all four rounds in a single U.S. Open\n",
      "championship.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Rory McIlroy's majors timeline: Breaking down golfer's last major ...\n",
      "All told, McIlroy has participated in 57 majors during his career. He has\n",
      "logged 28 top-10 finishes in those events: eight at the PGA Championship,\n",
      "seven at the U.S. Open, seven at the Masters and six at the British Open.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Rory McIlroy's majors timeline: Breaking down golfer's last major ...\n",
      "U.S. OPEN 2022: Tee times, pairings for first two days of major tournament\n",
      "## What major has Rory McIlroy not won?\n",
      "McIlroy has yet to win the Masters, though he has come close on several\n",
      "occasions. He finished second in 2022, with Scottie Scheffler denying the\n",
      "Northern Irishman the career Grand Slam.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Rory McIlroy's majors timeline: Breaking down golfer's last major ...\n",
      "MORE: Where Rory McIlroy ranks among golfers in the 2022 U.S. Open field\n",
      "## How many majors has Rory McIlroy won?\n",
      "McIlroy has won four major tournaments. They are as follows:\n",
      "* U.S. Open\n",
      "  * PGA Championship (twice)\n",
      "  * British Open\n",
      "## When was the last time Rory McIlroy won a major?\n",
      "McIlroy hasn't won a major championship in just about eight years. He emerged\n",
      "victoriously at the PGA Championship in 2014, at Valhalla Golf Club in\n",
      "Louisville, Ky.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Rory McIlroy's majors timeline: Breaking down golfer's last major ...\n",
      "McIlroy's best tournaments overall are the PGA Championship and the Masters.\n",
      "He has missed the cut a combined four times at those events and has won the\n",
      "PGA Championship twice. His best major season was in 2014, when he won the PGA\n",
      "and the British Open, finished tied for eighth at the Masters and tied for\n",
      "23rd at the U.S. Open.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "QUESTION (asked at 03/13/2024, 09:30:59 PT): how many times has rory mcilroy won the masters tournament?<|eot_id|><|start_header_id|>assistant<|end_header_id|>ANSWERABLE:\n",
      "############ Query: how many times has rory mcilroy won the masters tournament?\n",
      "############ answer: rory mcilroy has not won the masters tournament.\n",
      "############\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You will be provided with additional information between the \"<DOCS>\" tags. Keep your answers brief, ideally less than 20 words, but a strict limit of 30 words.\n",
      "\n",
      "If the provided information is insufficient for answering the question, simply output \"Insuffient information\" and only output that. If the question asserts a false premise, like \"When did Eisenhower become Prime Minister?\", simply output \"Invalid question\". Thus, if you are inclined to say something like \"X never has\" or \"X never was\" or \"X never did\", your output should be \"Invalid question\".\n",
      "\n",
      "Final reminder, with award shows, years can be tricky. Often awards are handed out the year after the project is made. Thus for the Oscars in 2015, the awards are being given out to movies made in 2014.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "# Example 1\n",
      "QUESTION: who was the first nba player to get a recorded triple double in basketball?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Andy Phillip got a triple-double versus the Fort Wayne Pistons on December 14, 1950.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: andy phillip\n",
      "\n",
      "# Example 2\n",
      "QUESTION: what year did ronald reagan become kommissar in the soviet union?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Ronald Wilson Reagan was an American politician and actor who served as the 40th president of the United States from 1981 to 1989.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Reagan's policies also helped contribute to the end of the Cold War and the end of Soviet communism.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: invalid question\n",
      "\n",
      "# Example 3\n",
      "QUESTION: what are the total number of farms in nebraska?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "There are many farms in Nebraska, some growing corn, while other growing sugar beats.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Nebraska's largest industries are the argiculture and cattle production industries.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: i don't know\n",
      "\n",
      "<DOCS>\n",
      "<DOC>\n",
      "2024 High Dividend Stocks List | Highest Yields Up To 27.7%\n",
      "### High Dividend Stock #7: Western Union (WU)\n",
      "* Dividend Yield: 6.8%\n",
      "  * Dividend Risk Score: C\n",
      "The Western Union Company is the world leader in the business of domestic and\n",
      "international money transfers. The company has a network of approximately\n",
      "550,000 agents globally and operates in more than 200 countries. About 90% of\n",
      "agents are outside of the US. Western Union operates two business segments,\n",
      "Consumer-to-Consumer (C2C) and Other (bill payments in the US and Argentina).\n",
      "</DOC>\n",
      "<DOC>\n",
      "9 Highest Dividend-Paying Stocks in the S&P 500 | Investing | U.S. ...\n",
      "## Altria Group Inc. (MO)\n",
      "* Market value: $72 billion\n",
      "  * Dividend yield: 9.6%\n",
      "Altria is a popular dividend stock that has been atop the list of the highest-\n",
      "paying S&P 500 dividend stocks for a while. That's because of its consistent\n",
      "and reliable yield and a great track record of increases in its payouts.\n",
      "Specifically, the stock has logged 55 consecutive years of dividend increases\n",
      "– longer than many other dividend stocks have even been in operation.  \n",
      "Of course, growth is challenged for this tobacco giant that produces Marlboro\n",
      "cigarettes, Black & Mild pipe and cigar products, and smokeless tobacco like\n",
      "Copenhagen and Skoal. But it has stable operations and dominates the U.S.\n",
      "market for those who are looking for a nicotine fix. If you want to trade\n",
      "growth potential for income potential, MO may be a stock that fits the bill.\n",
      "</DOC>\n",
      "<DOC>\n",
      "2024 High Dividend Stocks List | Highest Yields Up To 27.7%\n",
      "### High Dividend Stock #2: MPLX LP (MPLX)\n",
      "* Dividend Yield: 8.6%\n",
      "  * Dividend Risk Score: C\n",
      "MPLX LP is a Master Limited Partnership that was formed by the Marathon\n",
      "Petroleum Corporation (MPC) in 2012. In 2019, MPLX acquired Andeavor Logistics\n",
      "LP.\n",
      "</DOC>\n",
      "<DOC>\n",
      "9 Highest Dividend-Paying Stocks in the S&P 500 | Investing | U.S. ...\n",
      "## Pfizer Inc. (PFE)\n",
      "* Market value: $152 billion\n",
      "  * Dividend yield: 6.3%\n",
      "A relative newcomer to the lineup of top S&P 500 dividend stocks, Pfizer is a\n",
      "classic case of a company that has become a yield monster lately for all the\n",
      "wrong reasons. Yes, it has a long tradition of sharing the wealth with\n",
      "shareholders, with its first-quarter payday marking the 341st consecutive\n",
      "quarterly dividend paid by Pfizer – a track record that spans more than 85\n",
      "years.\n",
      "</DOC>\n",
      "<DOC>\n",
      "2024 High Dividend Stocks List | Highest Yields Up To 27.7%\n",
      "A company with a payout ratio over 100% is paying out more in dividends than\n",
      "it is making in profits, a long-term unsustainable situation. A company with a\n",
      "payout ratio of 50% is making double in income what it is paying out in\n",
      "dividends, so it has ‘room’ for earnings to decline significantly without\n",
      "reducing its dividend.\n",
      "</DOC>\n",
      "<DOC>\n",
      "2024 High Dividend Stocks List | Highest Yields Up To 27.7%\n",
      "Altria has increased its dividend for over 50 years, placing it on the\n",
      "exclusive Dividend Kings list. This is a rare business longevity achievement\n",
      "that speaks to the staying power of the company’s brands, even with the\n",
      "gradual decline in smoking in the U.S.\n",
      "</DOC>\n",
      "<DOC>\n",
      "2024 High Dividend Stocks List | Highest Yields Up To 27.7%\n",
      "Click on a company’s name to view the high dividend 50 series article for that\n",
      "company. A link to the specific Sure Analysis Research Database report page\n",
      "for each security is included as well.\n",
      "</DOC>\n",
      "<DOC>\n",
      "2024 High Dividend Stocks List | Highest Yields Up To 27.7%\n",
      "* High Dividend Stock #7: Western Union (WU)\n",
      "  * High Dividend Stock #6: Lincoln National Corp. (LNC)\n",
      "  * High Dividend Stock #5: Universal Health Realty Income Trust (UHT)\n",
      "  * High Dividend Stock #4: Enterprise Products Partners (EPD)\n",
      "  * High Dividend Stock #3: First of Long Island Corp. (FLIC)\n",
      "  * High Dividend Stock #2: MPLX LP (MPLX)\n",
      "  * High Dividend Stock #1: Altria Group (MO)\n",
      "</DOC>\n",
      "<DOC>\n",
      "2024 High Dividend Stocks List | Highest Yields Up To 27.7%\n",
      "### High Dividend Stock #1: Altria Group (MO)\n",
      "* Dividend Yield: 9.6%\n",
      "  * Dividend Risk Score: B\n",
      "Altria Group was founded by Philip Morris in 1847. Today, it is a consumer\n",
      "staples giant. It sells the Marlboro cigarette brand in the U.S. and a number\n",
      "of other non-smokeable brands, including Skoal and Copenhagen.\n",
      "</DOC>\n",
      "<DOC>\n",
      "High Dividend Stocks You Can Count On | Investor's Business Daily\n",
      "You might think no stocks can clear all these hurdles. Actually, eight did.\n",
      "## High Dividend Yield Winner: Rio Tinto\n",
      "When you've narrowed down the list of high-dividend stocks this much, it's OK\n",
      "to look for the top dividend yields. That title goes to Rio Tinto. Based in\n",
      "London, Rio Tinto mines and processes materials ranging from aluminum to\n",
      "copper and diamonds. And it yields a lucrative 9.7%. Additionally, the\n",
      "company's yield is up more than 34% over the past five years.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "QUESTION (asked at 03/13/2024, 08:56:12 PT): which companies have the highest level of dividend yield?<|eot_id|><|start_header_id|>assistant<|end_header_id|>ANSWERABLE:\n",
      "############ Query: which companies have the highest level of dividend yield?\n",
      "############ answer: Altria Group (MO) with 9.6% and Rio Tinto with 9.7% dividend yield.\n",
      "############\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You will be provided with additional information between the \"<DOCS>\" tags. Keep your answers brief, ideally less than 20 words, but a strict limit of 30 words.\n",
      "\n",
      "If the provided information is insufficient for answering the question, simply output \"Insuffient information\" and only output that. If the question asserts a false premise, like \"When did Eisenhower become Prime Minister?\", simply output \"Invalid question\". Thus, if you are inclined to say something like \"X never has\" or \"X never was\" or \"X never did\", your output should be \"Invalid question\".\n",
      "\n",
      "Final reminder, with award shows, years can be tricky. Often awards are handed out the year after the project is made. Thus for the Oscars in 2015, the awards are being given out to movies made in 2014.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "# Example 1\n",
      "QUESTION: who was the first nba player to get a recorded triple double in basketball?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Andy Phillip got a triple-double versus the Fort Wayne Pistons on December 14, 1950.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: andy phillip\n",
      "\n",
      "# Example 2\n",
      "QUESTION: what year did ronald reagan become kommissar in the soviet union?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Ronald Wilson Reagan was an American politician and actor who served as the 40th president of the United States from 1981 to 1989.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Reagan's policies also helped contribute to the end of the Cold War and the end of Soviet communism.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: invalid question\n",
      "\n",
      "# Example 3\n",
      "QUESTION: what are the total number of farms in nebraska?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "There are many farms in Nebraska, some growing corn, while other growing sugar beats.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Nebraska's largest industries are the argiculture and cattle production industries.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: i don't know\n",
      "\n",
      "<DOCS>\n",
      "\n",
      "</DOCS>\n",
      "QUESTION (asked at 02/26/2024, 23:52:36 PT): what is the shortest highway in the us in feet?<|eot_id|><|start_header_id|>assistant<|end_header_id|>ANSWERABLE:\n",
      "############ Query: what is the shortest highway in the us in feet?\n",
      "############ answer: Insufficient information\n",
      "############\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You will be provided with additional information between the \"<DOCS>\" tags. Keep your answers brief, ideally less than 20 words, but a strict limit of 30 words.\n",
      "\n",
      "If the provided information is insufficient for answering the question, simply output \"Insuffient information\" and only output that. If the question asserts a false premise, like \"When did Eisenhower become Prime Minister?\", simply output \"Invalid question\". Thus, if you are inclined to say something like \"X never has\" or \"X never was\" or \"X never did\", your output should be \"Invalid question\".\n",
      "\n",
      "Final reminder, with award shows, years can be tricky. Often awards are handed out the year after the project is made. Thus for the Oscars in 2015, the awards are being given out to movies made in 2014.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "# Example 1\n",
      "QUESTION: who was the first nba player to get a recorded triple double in basketball?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Andy Phillip got a triple-double versus the Fort Wayne Pistons on December 14, 1950.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: andy phillip\n",
      "\n",
      "# Example 2\n",
      "QUESTION: what year did ronald reagan become kommissar in the soviet union?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Ronald Wilson Reagan was an American politician and actor who served as the 40th president of the United States from 1981 to 1989.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Reagan's policies also helped contribute to the end of the Cold War and the end of Soviet communism.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: invalid question\n",
      "\n",
      "# Example 3\n",
      "QUESTION: what are the total number of farms in nebraska?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "There are many farms in Nebraska, some growing corn, while other growing sugar beats.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Nebraska's largest industries are the argiculture and cattle production industries.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: i don't know\n",
      "\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Biggest Companies in the World by Market Cap\n",
      "Companies are ranked as per TradingView's list of largest companies by market\n",
      "cap. All figures, which are also taken from TradingView, are current as of\n",
      "Jan. 20, 2024.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Biggest Companies in the World by Market Cap\n",
      "Saudi Arabian Oil, better known as Saudi Aramco, is a Saudi Arabia-based\n",
      "integrated oil and gas company. The company was founded as Standard Oil's\n",
      "overseas operations and is now owned by the Saudi government.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Biggest Companies in the World by Market Cap\n",
      "Tesla (TSLA) is a leading manufacturer of electric vehicles, which include the\n",
      "following models:\n",
      "* Model 3, a four-door, mid-size sedan designed and priced for mass-market appeal\n",
      "  * Model Y, a compact sport utility vehicle (SUV) that seats seven adults\n",
      "  * Model S, a four-door, full-size sedan\n",
      "  * Model X, a mid-size SUV with seating for up to seven adults\n",
      "</DOC>\n",
      "<DOC>\n",
      "Biggest Companies in the World by Market Cap\n",
      "Table of Contents\n",
      "\n",
      "Expand\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "  * 1\\. Microsoft\n",
      "\n",
      "  * 2\\. Apple\n",
      "\n",
      "  * 3\\. Saudi Aramco\n",
      "\n",
      "  * 4\\. Alphabet\n",
      "\n",
      "  * 5\\. Amazon\n",
      "\n",
      "  * 6\\. NVIDIA\n",
      "\n",
      "  * 7\\. Meta\n",
      "\n",
      "  * 8\\. Berkshire Hathaway\n",
      "\n",
      "  * 9\\. Tesla\n",
      "\n",
      "  * 10\\. Eli Lilly\n",
      "\n",
      "  * FAQs\n",
      "\n",
      "  * The Bottom Line\n",
      "</DOC>\n",
      "<DOC>\n",
      "Biggest Companies in the World by Market Cap\n",
      "Berkshire Hathaway (BRK.A/BRK.B) began with the merger of two regional textile\n",
      "companies in the 19th century. Only much later was it purchased by legendary\n",
      "investor Warren Buffett and converted into a conglomerate holding company.\n",
      "</DOC>\n",
      "<DOC>\n",
      "The Largest Companies by Market Cap in 2024 | The Motley Fool\n",
      "## Companies 16-20\n",
      "\n",
      "## 17\\. Walmart\n",
      "* Market cap: $444.89 billion (as of Feb. 1)\n",
      "  * Revenue (TTM): $638.79 billion\n",
      "  * Gross profit (TTM): $155.05 billion\n",
      "  * Five-year annualized return: 11.98%\n",
      "  * Year founded: 1962\n",
      "\n",
      "Walmart (NYSE:WMT) may not have the largest market cap, but it is No. 1 in\n",
      "terms of revenue, and it’s the largest retailer in the world. With more than\n",
      "$600 billion in annual revenue, it earns more than most of the other companies\n",
      "on this list by a wide margin.\n",
      "\n",
      "## 18\\. Mastercard\n",
      "</DOC>\n",
      "<DOC>\n",
      "The Largest Companies by Market Cap in 2024 | The Motley Fool\n",
      "* Log In\n",
      "* Help\n",
      "* Join The Motley Fool\n",
      "Top 10 Stocks\n",
      "Research  > Largest Companies By Market Cap\n",
      "\n",
      "# The Largest Companies by Market Cap in 2024\n",
      "\n",
      "## Microsoft, Apple, and Saudi Aramco are the biggest companies measured by\n",
      "market cap.\n",
      "\n",
      "By Lyle Daly – Updated Feb 2, 2024 at 11:24AM\n",
      "</DOC>\n",
      "<DOC>\n",
      "Biggest Companies in the World by Market Cap\n",
      "Berkshire Hathaway today owns a large number of subsidiaries engaged in\n",
      "insurance, freight rail transportation, retailing, and utility and energy\n",
      "generation and distribution. The company also operates manufacturing\n",
      "businesses that make products for industrial uses, construction, and for\n",
      "consumers.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Biggest Companies in the World by Market Cap\n",
      "The company offers products and platforms comprised of hardware, software,\n",
      "services, and more to serve the gaming, professional visualization, data\n",
      "center, and automotive markets. Nvidia GPUs have also featured prominently in\n",
      "cryptocurrency mining.\n",
      "</DOC>\n",
      "<DOC>\n",
      "The Largest Companies by Market Cap in 2024 | The Motley Fool\n",
      "After a strong start to 2024, Microsoft (NASDAQ:MSFT) is the most valuable\n",
      "company in the world. It passed longtime rival Apple (NASDAQ:AAPL) in January,\n",
      "when it also became just the second company to reach a market capitalization\n",
      "of $3 million. Seven of the largest companies have a market cap of at least $1\n",
      "trillion.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "QUESTION (asked at 02/28/2024, 08:35:24 PT): which company have larger market cap, plya or seatw?<|eot_id|><|start_header_id|>assistant<|end_header_id|>ANSWERABLE:\n",
      "############ Query: which company have larger market cap, plya or seatw?\n",
      "############ answer: insufficient information\n",
      "############\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You will be provided with additional information between the \"<DOCS>\" tags. Keep your answers brief, ideally less than 20 words, but a strict limit of 30 words.\n",
      "\n",
      "If the provided information is insufficient for answering the question, simply output \"Insuffient information\" and only output that. If the question asserts a false premise, like \"When did Eisenhower become Prime Minister?\", simply output \"Invalid question\". Thus, if you are inclined to say something like \"X never has\" or \"X never was\" or \"X never did\", your output should be \"Invalid question\".\n",
      "\n",
      "Final reminder, with award shows, years can be tricky. Often awards are handed out the year after the project is made. Thus for the Oscars in 2015, the awards are being given out to movies made in 2014.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "# Example 1\n",
      "QUESTION: who was the first nba player to get a recorded triple double in basketball?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Andy Phillip got a triple-double versus the Fort Wayne Pistons on December 14, 1950.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: andy phillip\n",
      "\n",
      "# Example 2\n",
      "QUESTION: what year did ronald reagan become kommissar in the soviet union?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Ronald Wilson Reagan was an American politician and actor who served as the 40th president of the United States from 1981 to 1989.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Reagan's policies also helped contribute to the end of the Cold War and the end of Soviet communism.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: invalid question\n",
      "\n",
      "# Example 3\n",
      "QUESTION: what are the total number of farms in nebraska?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "There are many farms in Nebraska, some growing corn, while other growing sugar beats.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Nebraska's largest industries are the argiculture and cattle production industries.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: i don't know\n",
      "\n",
      "<DOCS>\n",
      "<DOC>\n",
      "10 Actors Who Have Played A Comic Book Character In Both Live-Action ...\n",
      "##  Donald Glover\n",
      "Before Andrew Garfield's casting, a viral Twitter campaign suggested Donald\n",
      "Glover as the new incarnation of the friendly neighborhood. Glover didn't\n",
      "audition, but Miles Morales' creator, Brian Michael Bendis, did credit Glover\n",
      "with influencing the character's look after his Community character, Troy\n",
      "Barnes, appeared wearing a Spider-Man pajama.\n",
      "</DOC>\n",
      "<DOC>\n",
      "10 Actors Who Have Played A Comic Book Character In Both Live-Action ...\n",
      "Simmons would've played Commissioner Gordon in Ben Affleck's unrealized Batman\n",
      "movie. He returned to the role of Jameson for a small cameo at the end of\n",
      "2019's Spider-Man: Far from Home, and in 2020 began voicing Omni-Man in\n",
      "Amazon's animated series Invincible.\n",
      "</DOC>\n",
      "<DOC>\n",
      "10 Actors Who Have Played A Comic Book Character In Both Live-Action ...\n",
      "Not injecting Ads due to No-Ads mode.\n",
      "##  J.K. Simmons\n",
      "Academy Award winner J.K. Simmons rose to prominence with roles in Law and\n",
      "Order and Oz. In 2002, he began playing the role of J. Jonah Jameson in Sam\n",
      "Raimi's trilogy of Spider-Man films, which elevated his profile further. In\n",
      "2015, he won the Oscar for his supporting performance in Damien Chazelle's\n",
      "Whiplash.\n",
      "</DOC>\n",
      "<DOC>\n",
      "10 Actors Who Have Played A Comic Book Character In Both Live-Action ...\n",
      "Pine also had a small role in Into the Spiderverse, voicing the original Peter\n",
      "Parker in Miles' dimension. The character appears only at the beginning of the\n",
      "film, and Pine only speaks a few lines.\n",
      "</DOC>\n",
      "<DOC>\n",
      "10 Actors Who Have Played A Comic Book Character In Both Live-Action ...\n",
      "##  Mark Hamill\n",
      "Mark Hamill is a true legend when it comes to comic book characters. Known\n",
      "worldwide for his portrayal of Luke Skywalker in the Star Wars franchise,\n",
      "Hamill is also notorious for voicing the Joker in multiple projects across\n",
      "numerous forms of media.\n",
      "</DOC>\n",
      "<DOC>\n",
      "10 Actors Who Have Played A Comic Book Character In Both Live-Action ...\n",
      "##  Michael Cera\n",
      "After rising to stardom in the 2007 comedy Superbad, Michael Cera's career\n",
      "took off. He starred in movies like Year One and the Oscar-nominated Juno\n",
      "before taking on the role of Scott Pilgrim in 2010's Scott Pilgrim vs. the\n",
      "World, based on the comic book of the same name.\n",
      "</DOC>\n",
      "<DOC>\n",
      "10 Actors Who Have Played A Comic Book Character In Both Live-Action ...\n",
      "##  Chris Pine\n",
      "After making his big-screen debut in The Princess Diaries 2: Royal Engagement,\n",
      "Chris Pine's profile rose to leading man status. He played James T. Kirk in\n",
      "the Star Trek reboot films and took on the role of Steve Trevor in 2017's\n",
      "Wonder Woman and its sequel, Wonder Woman 1984.\n",
      "</DOC>\n",
      "<DOC>\n",
      "10 Actors Who Have Played A Comic Book Character In Both Live-Action ...\n",
      "In 2021, Yeun received his first Oscar nomination for his leading role in Lee\n",
      "Isaac Chung's Minari. He'll return as Invincible's voice for the show's\n",
      "recently announced second season.\n",
      "</DOC>\n",
      "<DOC>\n",
      "10 Actors Who Have Played A Comic Book Character In Both Live-Action ...\n",
      "The actor also played James Jesse, AKA The Trickster, in the 90s TV series The\n",
      "Flash. He reprised the role in The CW show, The Flash, fourteen years after\n",
      "first playing him.\n",
      "</DOC>\n",
      "<DOC>\n",
      "10 Actors Who Have Played A Comic Book Character In Both Live-Action ...\n",
      "##  Mahershala Ali\n",
      "Two-time Oscar winner Mahershala Ali had a stratospheric rise to stardom in\n",
      "the second half of the 2010s. He won his first Oscar for his performance in\n",
      "2016's Moonlight. In 2018, he voiced Aaron Davis, AKA, Prowler in the\n",
      "critically acclaimed, Oscar-winning movie Spider-Man: Into the Spiderverse,\n",
      "and won his second Oscar for Green Book.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "QUESTION (asked at 03/13/2024, 10:39:22 PT): who was the first actor to play the role of a comic book villain in a live-action movie?<|eot_id|><|start_header_id|>assistant<|end_header_id|>ANSWERABLE:\n",
      "############ Query: who was the first actor to play the role of a comic book villain in a live-action movie?\n",
      "############ answer: Insufficient information\n",
      "############\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You will be provided with additional information between the \"<DOCS>\" tags. Keep your answers brief, ideally less than 20 words, but a strict limit of 30 words.\n",
      "\n",
      "If the provided information is insufficient for answering the question, simply output \"Insuffient information\" and only output that. If the question asserts a false premise, like \"When did Eisenhower become Prime Minister?\", simply output \"Invalid question\". Thus, if you are inclined to say something like \"X never has\" or \"X never was\" or \"X never did\", your output should be \"Invalid question\".\n",
      "\n",
      "Final reminder, with award shows, years can be tricky. Often awards are handed out the year after the project is made. Thus for the Oscars in 2015, the awards are being given out to movies made in 2014.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "# Example 1\n",
      "QUESTION: who was the first nba player to get a recorded triple double in basketball?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Andy Phillip got a triple-double versus the Fort Wayne Pistons on December 14, 1950.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: andy phillip\n",
      "\n",
      "# Example 2\n",
      "QUESTION: what year did ronald reagan become kommissar in the soviet union?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "Ronald Wilson Reagan was an American politician and actor who served as the 40th president of the United States from 1981 to 1989.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Reagan's policies also helped contribute to the end of the Cold War and the end of Soviet communism.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: invalid question\n",
      "\n",
      "# Example 3\n",
      "QUESTION: what are the total number of farms in nebraska?\n",
      "<DOCS>\n",
      "<DOC>\n",
      "There are many farms in Nebraska, some growing corn, while other growing sugar beats.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Nebraska's largest industries are the argiculture and cattle production industries.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "ANSWER: i don't know\n",
      "\n",
      "<DOCS>\n",
      "<DOC>\n",
      "DreamWorks Pictures | Dreamworks Animation Wiki | Fandom\n",
      "United International Pictures (a joint venture of Paramount and Universal)\n",
      "formerly distributed most DreamWorks' films internationally.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Universal Pictures | Dreamworks Animation Wiki | Fandom\n",
      "Universal itself was the film (outside of America) and video distributor for\n",
      "DreamWorks live-action titles and DWA titles (Back when DreamWorks Animation\n",
      "was a division of DW before it was spun off in 2004), DreamWorks' theatrical\n",
      "films from 1998-2006 in American however were distributed by Universal, but\n",
      "through their then DreamWorks Pictures label. DreamWorks was then sold to\n",
      "Paramount Pictures in 2006, at which point Paramount took over distribution.\n",
      "</DOC>\n",
      "<DOC>\n",
      "DreamWorks Pictures | Dreamworks Animation Wiki | Fandom\n",
      "DreamWorks began in 1994 as an attempt by media moguls Steven Spielberg,\n",
      "Jeffrey Katzenberg and David Geffen (together, SKG) to create a new Hollywood\n",
      "studio of which they owned 72%. Currently, DreamWorks operates out of offices\n",
      "at Universal Studios.\n",
      "</DOC>\n",
      "<DOC>\n",
      "DreamWorks Pictures | Dreamworks Animation Wiki | Fandom\n",
      "* ↑ 6.0 6.1 Comcast's NBCUniversal completes purchase of DreamWorks Animation. Retrieved on 23 August 2016.\n",
      "</DOC>\n",
      "<DOC>\n",
      "DreamWorks Pictures - Wikipedia\n",
      "* ^ \"Comcast's NBCUniversal completes purchase of DreamWorks Animation\". Los Angeles Times. Retrieved 23 August 2016.\n",
      "</DOC>\n",
      "<DOC>\n",
      "DreamWorks Pictures | Dreamworks Animation Wiki | Fandom\n",
      "DreamWorks Pictures (also known as DreamWorks SKG or DreamWorks Studios,\n",
      "commonly referred to as DreamWorks) is an American film production label of\n",
      "Amblin Partners. It was formerly distributing its own and third-party films by\n",
      "itself. It has produced or distributed more than ten films with box-office\n",
      "grosses of more than $100 million each. As of October 2016, DreamWorks' films\n",
      "are marketed and distributed by Universal Pictures.\n",
      "</DOC>\n",
      "<DOC>\n",
      "Universal Pictures | Dreamworks Animation Wiki | Fandom\n",
      "Google Tag Manager (noscript)\n",
      "End Google Tag Manager (noscript)\n",
      "Studio's current logo since 2013.\n",
      "Universal Pictures (also known as Universal Studios) is an American film\n",
      "studio founded in 1912. It was owned by NBCUniversal, a division of Comcast.\n",
      "</DOC>\n",
      "<DOC>\n",
      "DreamWorks Pictures | Dreamworks Animation Wiki | Fandom\n",
      "DreamWorks' animation arm was spun off in 2004 into DreamWorks Animation SKG\n",
      "(DWA), which currently owns the DreamWorks trademarks, and as of August 2016\n",
      "is a subsidiary of NBCUniversal.[6] Spielberg's company continues to use the\n",
      "DreamWorks trademarks under license from Universal Studios.[7][8]\n",
      "</DOC>\n",
      "<DOC>\n",
      "DreamWorks Pictures | Dreamworks Animation Wiki | Fandom\n",
      "In August 2016, NBCUniversal, whose subsidiary Universal Pictures entered a\n",
      "distribution deal with the live-action DreamWorks studio via Amblin Partners\n",
      "in December 2015, acquired DreamWorks Animation for $3.8 billion.[63][6]\n",
      "DreamWorks Animation's distribution contract with 20th Century Fox will\n",
      "conclude in 2019, after which Universal is expected to handle future releases,\n",
      "starting with How to Train Your Dragon 3 .[64][65]\n",
      "</DOC>\n",
      "<DOC>\n",
      "Universal Pictures | Dreamworks Animation Wiki | Fandom\n",
      "After Viacom spun off DreamWorks Studios in 2008, Universal was planned to\n",
      "resume distributing DreamWorks' movies, but this deal fell through as Disney\n",
      "took over the distribution for DreamWorks movies under the Touchstone Pictures\n",
      "banner starting in 2011 until 2016. In April 2016, Comcast (which owns\n",
      "Universal) purchased DreamWorks Animation for $3.8 Billion. It is therefore\n",
      "likely that when the current distribution deal with 20th Century Fox finishes,\n",
      "DreamWorks Animation's films will be distributed by Universal. How to Train\n",
      "Your Dragon: The Hidden World (2019) was the first DWA film distributed by\n",
      "Universal and as a Universal subsidiary.\n",
      "</DOC>\n",
      "</DOCS>\n",
      "QUESTION (asked at 03/10/2024, 23:34:42 PT): is dreamworks animation owned by time warner or universal pictures?<|eot_id|><|start_header_id|>assistant<|end_header_id|>ANSWERABLE:\n",
      "############ Query: is dreamworks animation owned by time warner or universal pictures?\n",
      "############ answer: universal pictures\n",
      "############\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ccbd835b97741648c6d41b5b91254bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Predictions:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-19 15:21:55.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluate_predictions\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1m{'score': -0.4, 'exact_accuracy': 0.2, 'accuracy': 0.3, 'hallucination': 0.7, 'missing': 0.0, 'n_miss': 0, 'n_correct': 3, 'n_correct_exact': 2, 'total': 10}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from loguru import logger\n",
    "from openai import APIConnectionError, OpenAI, RateLimitError\n",
    "from prompts.templates import IN_CONTEXT_EXAMPLES, INSTRUCTIONS\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import LlamaTokenizerFast\n",
    "\n",
    "os.environ[\"INTERWEB_APIKEY\"] = \"D40tLQc4plxanXT91P3zEJ2Dk1mjNUJOhjxT7uCKkZPgFq1NO1Ew8ZLI47KICpku\"\n",
    "# os.environ['INTERWEB_HOST'] = \"http://gpunode04.kbs:11434/v1/\"\n",
    "os.environ['INTERWEB_HOST'] = \"https://interweb.l3s.uni-hannover.de/v1\"\n",
    "# os.environ['INTERWEB_APIKEY'] = \"ollama\"\n",
    "\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(\"tokenizer\")\n",
    "\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"Load and return the content of a JSON file.\"\"\"\n",
    "    logger.info(f\"Loading JSON from {file_path}\")\n",
    "    with open(file_path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def get_system_message():\n",
    "    \"\"\"Returns the system message containing instructions and in context examples.\"\"\"\n",
    "    return INSTRUCTIONS + IN_CONTEXT_EXAMPLES\n",
    "\n",
    "\n",
    "def attempt_api_call(client, model_name, messages, max_retries=10):\n",
    "    \"\"\"Attempt an API call with retries upon encountering specific errors.\"\"\"\n",
    "    # todo: add default response when all efforts fail\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=messages,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except (APIConnectionError, RateLimitError):\n",
    "            logger.warning(\n",
    "                f\"API call failed on attempt {attempt + 1}, retrying...\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {e}\")\n",
    "            break\n",
    "    return None\n",
    "\n",
    "\n",
    "def log_response(messages, response, output_directory=\"api_responses\"):\n",
    "    \"\"\"Save the response from the API to a file.\"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    file_name = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S.json\")\n",
    "    file_path = os.path.join(output_directory, file_name)\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump({\"messages\": messages, \"response\": response}, f)\n",
    "\n",
    "\n",
    "def parse_response(resp: str):\n",
    "    \"\"\"Pass auto-eval output from the evaluator.\"\"\"\n",
    "    try:\n",
    "        resp = resp.lower()\n",
    "        model_resp = json.loads(resp)\n",
    "        answer = -1\n",
    "        if \"accuracy\" in model_resp and (\n",
    "            (model_resp[\"accuracy\"] is True)\n",
    "            or (\n",
    "                isinstance(model_resp[\"accuracy\"], str)\n",
    "                and model_resp[\"accuracy\"].lower() == \"true\"\n",
    "            )\n",
    "        ):\n",
    "            answer = 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Could not parse answer from response: {model_resp}\"\n",
    "            )\n",
    "\n",
    "        return answer\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def trim_predictions_to_max_token_length(prediction):\n",
    "    \"\"\"Trims prediction output to 75 tokens\"\"\"\n",
    "    max_token_length = 75\n",
    "    tokenized_prediction = tokenizer.encode(prediction)\n",
    "    trimmed_tokenized_prediction = tokenized_prediction[\n",
    "        1 : max_token_length + 1\n",
    "    ]\n",
    "    trimmed_prediction = tokenizer.decode(trimmed_tokenized_prediction)\n",
    "    return trimmed_prediction\n",
    "\n",
    "\n",
    "def generate_predictions(dataset_path, participant_model):\n",
    "    predictions = []\n",
    "    with bz2.open(DATASET_PATH, \"rt\") as bz2_file:\n",
    "        for line in tqdm(bz2_file, desc=\"Generating Predictions\"):\n",
    "            data = json.loads(line)\n",
    "\n",
    "            query = data[\"query\"]\n",
    "            web_search_results = data[\"search_results\"]\n",
    "            query_time = data[\"query_time\"]\n",
    "\n",
    "            prediction = participant_model.generate_answer(\n",
    "                query, web_search_results, query_time\n",
    "            )\n",
    "\n",
    "            # trim prediction to 75 tokens\n",
    "            prediction = trim_predictions_to_max_token_length(prediction)\n",
    "            predictions.append(\n",
    "                {\n",
    "                    \"query\": query,\n",
    "                    \"ground_truth\": str(data[\"answer\"]).strip().lower(),\n",
    "                    \"prediction\": str(prediction).strip().lower(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def evaluate_predictions(predictions, evaluation_model_name, openai_client):\n",
    "    n_miss, n_correct, n_correct_exact = 0, 0, 0\n",
    "    system_message = get_system_message()\n",
    "\n",
    "    for prediction_dict in tqdm(\n",
    "        predictions, total=len(predictions), desc=\"Evaluating Predictions\"\n",
    "    ):\n",
    "        query, ground_truth, prediction = (\n",
    "            prediction_dict[\"query\"],\n",
    "            prediction_dict[\"ground_truth\"],\n",
    "            prediction_dict[\"prediction\"],\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Question: {query}\\n Ground truth: {ground_truth}\\n Prediction: {prediction}\\n\",\n",
    "            },\n",
    "        ]\n",
    "        if prediction == \"i don't know\" or prediction == \"i don't know.\":\n",
    "            n_miss += 1\n",
    "            continue\n",
    "        if prediction == ground_truth:\n",
    "            n_correct_exact += 1\n",
    "            n_correct += 1\n",
    "            continue\n",
    "\n",
    "        response = attempt_api_call(\n",
    "            openai_client, evaluation_model_name, messages\n",
    "        )\n",
    "        if response:\n",
    "            log_response(messages, response)\n",
    "            eval_res = parse_response(response)\n",
    "            if eval_res == 1:\n",
    "                n_correct += 1\n",
    "\n",
    "    n = len(predictions)\n",
    "    results = {\n",
    "        \"score\": (2 * n_correct + n_miss) / n - 1,\n",
    "        \"exact_accuracy\": n_correct_exact / n,\n",
    "        \"accuracy\": n_correct / n,\n",
    "        \"hallucination\": (n - n_correct - n_miss) / n,\n",
    "        \"missing\": n_miss / n,\n",
    "        \"n_miss\": n_miss,\n",
    "        \"n_correct\": n_correct,\n",
    "        \"n_correct_exact\": n_correct_exact,\n",
    "        \"total\": n,\n",
    "    }\n",
    "    logger.info(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from models.user_config import UserModel\n",
    "\n",
    "    DATASET_PATH = \"example_data/dev_data.jsonl.bz2\"\n",
    "    # EVALUATION_MODEL_NAME = os.getenv(\n",
    "    #     \"EVALUATION_MODEL_NAME\", \"gpt-4-0125-preview\"\n",
    "    # )\n",
    "    EVALUATION_MODEL_NAME = os.getenv(\"EVALUATION_MODEL_NAME\", \"gemma2:27b\")\n",
    "    # Generate predictions\n",
    "    participant_model = UserModel()\n",
    "    predictions = generate_predictions(DATASET_PATH, participant_model)\n",
    "\n",
    "    # Evaluate Predictions\n",
    "    # openai_client = OpenAI()\n",
    "    openai_client = OpenAI(\n",
    "        base_url=os.getenv(\"INTERWEB_HOST\", \"https://interweb.l3s.uni-hannover.de\"),\n",
    "        api_key=os.getenv(\"INTERWEB_APIKEY\")\n",
    "    )\n",
    "    evaluation_results = evaluate_predictions(\n",
    "        predictions, EVALUATION_MODEL_NAME, openai_client\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7801c8a8ee44bcfbc0b09aff3915d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "from src.paths import MODELS_DIR\n",
    "\n",
    "DEFAULT_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"INTERWEB_APIKEY\"] = \"D40tLQc4plxanXT91P3zEJ2Dk1mjNUJOhjxT7uCKkZPgFq1NO1Ew8ZLI47KICpku\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=os.getenv(\"INTERWEB_HOST\", \"https://interweb.l3s.uni-hannover.de/v1\"),\n",
    "    api_key=os.getenv(\"INTERWEB_APIKEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "# DEFAULT_SYSTEM_INSTRUCTIONS = \"\"\"\n",
    "# You are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You should output only the necessary information to answer the question, i.e. be brief.\n",
    "# You will be given additional context retrieved from relevant webpages to your question beteen \"<DOCS>\" and \"</DOCS>\". Below is a couple of examples of this:\n",
    "\n",
    "# ### EXAMPLE ONLY\n",
    "# QUESTION: how much further can bald eagles see than humans?\n",
    "# <DOCS>\n",
    "# <DOC> They have amazing eyesight: A human with perfect eyesight has 20/20 vision. Bald eagles can have 20/4 or 20/5 vision, meaning they can see four or five times farther than the average person.</DOC>\n",
    "# <DOC> There are many metrics to compare. For example, Human eyes take up only 5% of our head, eagle eyes occupy 50%. The back of their eyes is bigger and flatter, physically allowing for a bigger picture on their retinas, which are more densely covered in light detector cells (cones). Eagles see clearly for up to 5x further than a human with perfect vision.</DOC>\n",
    "# </DOCS>\n",
    "# ANSWER: Up to 4 to 5 times as far as an average human.\n",
    "\n",
    "# QUESTION: which state was joe biden born?\n",
    "# <DOCS>\n",
    "# <DOC> Early life: Joseph Robinette Biden Jr. was born on November 20, 1942, at St. Mary's Hospital in Scranton, Pennsylvania, to Catherine Eugenia \"Jean\" Biden (née Finnegan) and Joseph Robinette Biden Sr. The oldest child in a Catholic family of English, French, and Irish descent, he has a sister, Valerie, and two brothers, Francis and James.</DOC>\n",
    "# </DOCS>\n",
    "# ANSWER: Pennsylvania.\n",
    "# ### EXAMPLE ONLY\n",
    "\n",
    "# You must base your outputs on the information within <DOCS> for the particular answer, e.g. do not simply output one of the answers above \n",
    "# \"\"\".strip()\n",
    "\n",
    "QA_SYSTEM_INSTRUCTIONS = \"\"\"\n",
    "You are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You will be provided with additional information between the \"<DOCS>\" tags. Keep your answers brief, ideally less than 20 words, but a strict limit of 30 words.\n",
    "\n",
    "If the provided information is insufficient for answering the question, simply output \"Insuffient information\" and only output that. If the question asserts a false premise, like \"When did Eisenhower become Prime Minister?\", simply output \"Invalid question\". Thus, if you are inclined to say something like \"X never has\" or \"X never was\" or \"X never did\", your output should be \"Invalid question\".\n",
    "\n",
    "Final reminder, with award shows, years can be tricky. Often awards are handed out the year after the project is made. Thus for the Oscars in 2015, the awards are being given out to movies made in 2014.\n",
    "\"\"\".strip()\n",
    "\n",
    "# QA_SYSTEM_INSTRUCTIONS = \"\"\"\n",
    "# You are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You will be provided with additional information between the \"<DOCS>\" tags. Keep your answers brief, ideally less than 20 words, but a strict limit of 30 words.\n",
    "\n",
    "# If the provided information is insufficient for answering the question, simply output \"Insuffient information\" and only output that. If the question asserts a false premise, like \"When did Eisenhower become Prime Minister?\", simply output \"Invalid question\". Thus, if you are inclined to say something like \"X never has\" or \"X never was\" or \"X never did\", your output should be \"Invalid question\".\n",
    "\n",
    "# Final reminder, with award shows, years can be tricky. Often awards are handed out the year after the project is made. Thus for the Oscars in 2015, the awards are being given out to movies made in 2014.\n",
    "\n",
    "# If the question is asking you to report a stock price, market cap, or which team played which recently, simply say \"I don't know\" as you don't have enough information to answer those questions. \n",
    "\n",
    "# When the query asks for a date give both the day and month if you can. If it asks 'what year' then just returning the year is fine. Finally, answer all questions fully, you tend to only give partial answers to questions that require multiple people for the answer.\n",
    "# \"\"\".strip()\n",
    "\n",
    "# QA_SYSTEM_INSTRUCTIONS = \"\"\"\n",
    "# You are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You will be provided with additional information between the \"<DOCS>\" tags. Keep your answers brief, ideally less than 20 words, but a strict limit of 30 words.\n",
    "\n",
    "# If the provided information is insufficient for answering the question, simply output \"Insuffient information\" and only output that. If the question asserts a false premise, like \"When did Eisenhower become Prime Minister?\", simply output \"Invalid question\". Thus, if you are inclined to say something like \"X never has\" or \"X never was\" or \"X never did\", your output should be \"Invalid question\".\n",
    "\n",
    "# Final reminder, with award shows, years can be tricky. Often awards are handed out the year after the project is made. Thus for the Oscars in 2015, the awards are being given out to movies made in 2014. Finally, your answer must be returned in lower case.\n",
    "# \"\"\".strip()\n",
    "\n",
    "# QA_SYSTEM_INSTRUCTIONS = \"\"\"\n",
    "# You are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You will be provided with additional information between the \"<DOCS>\" tags. Keep your answers brief, ideally less than 20 words, but a strict limit of 30 words.\n",
    "\n",
    "# Note, the scoring for this task means the making predictions when you are unsure you are punished. There are two alternative answers you may give other than the answer:\\nI don't know: Use this when the information provided is insufficient for answering the question\\nInvalid question: Use this when the question asserts a false premise, for example \"When did Ronald Reagan become the Prime Minister of Russia\"\\n\\nI\n",
    "# \"\"\".strip()\n",
    "\n",
    "QA_SYSTEM_INSTRUCTIONS_NO_CONTEXT = \"\"\"\n",
    "You are a helpful, respectful and honest question answering system. Your job is to answer questions that you can answer without any additional context. This includes topics you have knowledge off, like simple facts that you know, as well as reasoning and common sense questions. Please keep your answers brief, they must be less than 50 word answers.\n",
    "\n",
    "If the question asserts a false premise, like \"When did Eisenhower become Prime Minister?\", simply output \"Invalid question\".\n",
    "\n",
    "If you do not know the answer, simple output \"I don't know\".\n",
    "\n",
    "Also for date related questions (e.g. what date did X happen) simply return \"I don't know\" as you have a hard time giving the correct dates.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "API_SYSTEM_INSTRUCTIONS = \"\"\"\n",
    "You are a system that predicts the correct Python function to call with the associated arguments. You will be given a query and based on that query, you will be given the documentation for a set of Python functions that call a remote API and return the results. Your job is to take the query and predict the correct function call and arguments based on the query.\n",
    "Your response will be piped directly into Python's `eval()`, so only include the function and arguments with no additional text, otherwise you will cause an exception in the code. If you think none of the API functions can provide relevant information, simple return \"None\".\n",
    "\"\"\".strip()\n",
    "\n",
    "API_DOCS = \"\"\"\n",
    "<DOCUMENTATION>\n",
    "open_search_entity_by_name(query)\n",
    "    Get details about the list of entities returned by query.\n",
    "    Args:\n",
    "        query (str): the query\n",
    "    Returns:\n",
    "        A list of entities (List[str])\n",
    "\n",
    "\n",
    "movie_get_person_info(query)\n",
    "    Gets person info in database through BM25.\n",
    "    Args:\n",
    "        query (str): person name to be searched\n",
    "    Returns:\n",
    "        list of top n matching entities (List[Dict[str, Any]]). Entities are ranked by BM25 score. The returned entities MAY contain the following fields:\n",
    "            name (string): name of person\n",
    "            id (int): unique id of person\n",
    "            acted_movies (list[int]): list of movie ids in which person acted\n",
    "            directed_movies (list[int]): list of movie ids in which person directed\n",
    "            birthday (string): string of person's birthday, in the format of \"YYYY-MM-DD\"\n",
    "            oscar_awards: list of oscar awards (dict), win or nominated, in which the person was the entity. The format for oscar award entity are:\n",
    "                'year_ceremony' (int): year of the oscar ceremony,\n",
    "                'ceremony' (int): which ceremony. for example, ceremony = 50 means the 50th oscar ceremony,\n",
    "                'category' (string): category of this oscar award,\n",
    "                'name' (string): name of the nominee,\n",
    "                'film' (string): name of the film,\n",
    "                'winner' (bool): whether the person won the award\n",
    "\n",
    "\n",
    "movie_get_movie_info(query)\n",
    "    Gets movie info in database through BM25.\n",
    "    Args:\n",
    "        query (str): movie name to be searched\n",
    "    Returns:\n",
    "        list of top n matching entities (List[Dict[str, Any]]). Entities are ranked by BM25 score. The returned entities MAY contain the following fields:\n",
    "            title (string): title of movie\n",
    "            id (int): unique id of movie\n",
    "            release_date (string): string of movie's release date, in the format of \"YYYY-MM-DD\"\n",
    "            original_title (string): original title of movie, if in another language other than english\n",
    "            original_language (string): original language of movie. Example: 'en', 'fr'\n",
    "            budget (int): budget of movie, in USD\n",
    "            revenue (int): revenue of movie, in USD\n",
    "            rating (float): rating of movie, in range [0, 10]\n",
    "            genres (list[dict]): list of genres of movie. Sample genre object is {'id': 123, 'name': 'action'}\n",
    "            oscar_awards: list of oscar awards (dict), win or nominated, in which the movie was the entity. The format for oscar award entity are:\n",
    "                'year_ceremony' (int): year of the oscar ceremony,\n",
    "                'ceremony' (int): which ceremony. for example, ceremony = 50 means the 50th oscar ceremony,\n",
    "                'category' (string): category of this oscar award,\n",
    "                'name' (string): name of the nominee,\n",
    "                'film' (string): name of the film,\n",
    "                'winner' (bool): whether the person won the award\n",
    "            cast (list [dict]): list of cast members of the movie and their roles. The format of the cast member entity is:\n",
    "                'name' (string): name of the cast member,\n",
    "                'id' (int): unique id of the cast member,\n",
    "                'character' (string): character played by the cast member in the movie,\n",
    "                'gender' (string): the reported gender of the cast member. Use 2 for actor and 1 for actress,\n",
    "                'order' (int): order of the cast member in the movie. For example, the actress with the lowest order is the main actress,\n",
    "            crew' (list [dict]): list of crew members of the movie and their roles. The format of the crew member entity is:\n",
    "                'name' (string): name of the crew member,\n",
    "                'id' (int): unique id of the crew member,\n",
    "                'job' (string): job of the crew member,\n",
    "\n",
    "\n",
    "movie_get_year_info(query)\n",
    "    Gets info of a specific year\n",
    "    Args:\n",
    "        query (str): string of year. Note that we only support years between 1990 and 2021\n",
    "    Returns:\n",
    "        An entity representing year information (Dict[str, Any]). The returned entity MAY contain the following fields:\n",
    "            movie_list: list of movie ids in the year. This field can be very long to a few thousand films\n",
    "            oscar_awards: list of oscar awards (dict), held in that particular year. The format for oscar award entity are:\n",
    "                'year_ceremony' (int): year of the oscar ceremony,\n",
    "                'ceremony' (int): which ceremony. for example, ceremony = 50 means the 50th oscar ceremony,\n",
    "                'category' (string): category of this oscar award,\n",
    "                'name' (string): name of the nominee,\n",
    "                'film' (string): name of the film,\n",
    "                'winner' (bool): whether the person won the award\n",
    "\n",
    "\n",
    "finance_get_price_history_by_ticker(query)\n",
    "    Return 1 year history of daily Open price, Close price, High price, Low price and trading Volume.\n",
    "    Args:\n",
    "        query (str): ticker_name\n",
    "    Returns:\n",
    "        1 year daily price histor)\n",
    "\n",
    "\n",
    "finance_get_detailed_price_history_by_ticker(query)\n",
    "    Return the past 5 days' history of 1 minute Open price, Close price, High price, Low price and trading Volume, starting from 09:30:00 EST to 15:59:00 EST. Note that the Open, Close, High, Low, Volume are the data for the 1 min duration. However, the Open at 9:30:00 EST may not be equal to the daily Open price, and Close at 15:59:00 EST may not be equal to the daily Close price, due to handling of the paper trade. The sum of the 1 minute Volume may not be equal to the daily Volume.\n",
    "    Args:\n",
    "        query (str): ticker_name\n",
    "    Returns:\n",
    "        Past 5 days' 1 min price history\n",
    "\n",
    "\n",
    "finance_get_detailed_price_history_by_name(query)\n",
    "    Return the past 5 days' history of 1 minute Open price, Close price, High price, Low price and trading Volume, starting from 09:30:00 EST to 15:59:00 EST. Note that the Open, Close, High, Low, Volume are the data for the 1 min duration. However, the Open at 9:30:00 EST may not be equal to the daily Open price, and Close at 15:59:00 EST may not be equal to the daily Close price, due to handling of the paper trade. The sum of the 1 minute Volume may not be equal to the daily Volume.\n",
    "    Args:\n",
    "        query (str): company_name\n",
    "    Returns:\n",
    "        Past 5 days' 1 min price history\n",
    "\n",
    "\n",
    "finance_get_divdends_history_by_ticker(query)\n",
    "    Return dividend history of a ticker.\n",
    "    Args:\n",
    "        query (str): ticker_name\n",
    "    Returns:\n",
    "        Dividend distribution history\n",
    "\n",
    "\n",
    "finance_get_divdends_history_by_name(query)\n",
    "    Return dividend history of a ticker searching by company name.\n",
    "    Args:\n",
    "        query (str): company name\n",
    "    Returns:\n",
    "        Dividend distribution history whose format follows the below example: {'2019-12-19 00:00:00 EST': 0.058, '2020-03-19 00:00:00 EST': 0.2, '2020-06-12 00:00:00 EST': 0.2, ... }\n",
    "\n",
    "\n",
    "finance_get_market_capitalization_by_ticker(query)\n",
    "    Return the market capitalization of a ticker.\n",
    "    Args:\n",
    "        query (str): ticker_name\n",
    "    Returns:\n",
    "        Market capitalization (float)\n",
    "\n",
    "\n",
    "finance_get_market_capitalization_by_name(query)\n",
    "    Return the market capitalization of a ticker searching by company name.\n",
    "    Args:\n",
    "        query (str): company_name\n",
    "    Returns:\n",
    "        Market capitalization (float)\n",
    "\n",
    "\n",
    "finance_get_eps_by_ticker(query)\n",
    "    Return earnings per share of a ticker.\n",
    "    Args:\n",
    "        query (str): ticker_name\n",
    "    Returns:\n",
    "        Earnings per share (float)\n",
    "\n",
    "\n",
    "finance_get_eps_by_name(query)\n",
    "    Return earnings per share of a ticker searching by company name.\n",
    "    Args:\n",
    "        query (str): company_name\n",
    "    Returns:\n",
    "        Earnings per share (float)\n",
    "\n",
    "\n",
    "finance_get_pe_ratio_by_ticker(query)\n",
    "    Return price-to-earnings ratio of a ticker.\n",
    "    Args:\n",
    "        query (str): ticker_name\n",
    "    Returns:\n",
    "        Price-to-earnings ratio (float)\n",
    "\n",
    "\n",
    "finance_get_pe_ratio_by_name(query)\n",
    "    Return price-to-earnings ratio of a ticker searching by company name.\n",
    "    Args:\n",
    "        query (str): company_name\n",
    "    Returns:\n",
    "        Price-to-earnings ratio (float)\n",
    "\n",
    "\n",
    "finance_get_info_by_ticker(query)\n",
    "    Return meta data of a ticker.\n",
    "    Args:\n",
    "        query (str): ticker_name:\n",
    "    Returns:\n",
    "        General information regarding the company.\n",
    "\n",
    "\n",
    "finance_get_info_by_name(query)\n",
    "    Return meta data of a ticker searching by company name.\n",
    "    Args:\n",
    "        query (str): company_name:\n",
    "    Returns:\n",
    "        General information regarding the company.\n",
    "\n",
    "\n",
    "music_search_artist_entity_by_name(query)\n",
    "    Return the fuzzy matching results of the query (artist name).\n",
    "    Args:\n",
    "        query (str): artist name\n",
    "    Returns:\n",
    "        Top-10 similar entity name in a list\n",
    "\n",
    "\n",
    "music_search_song_entity_by_name(query)\n",
    "    Return the fuzzy matching results of the query (song name).\n",
    "    Args:\n",
    "        query (str): song name\n",
    "    Returns:\n",
    "        Top-10 similar entity name in a list\n",
    "\n",
    "\n",
    "music_get_billboard_rank_date(rank, date='')\n",
    "    Return the song name(s) and the artist name(s) of a certain rank on a certain date; If no date is given, return the list of of a certain rank of all dates.\n",
    "    Args:\n",
    "        rank (int): the interested rank in billboard; from 1 to 100.\n",
    "        date (Optional, str, in YYYY-MM-DD format): the interested date; leave it blank if do not want to specify the date.\n",
    "    Returns:\n",
    "        rank_list (list): a list of song names of a certain rank (on a certain date).\n",
    "        artist_list (list): a list of author names corresponding to the song names returned.\n",
    "\n",
    "\n",
    "music_get_billboard_attributes(date, attribute, song_name)\n",
    "    Return the attributes of a certain song on a certain date\n",
    "    Args:\n",
    "        date (str, in YYYY-MM-DD format): the interested date of the song\n",
    "        attribute (str): attributes from ['rank_last_week', 'weeks_in_chart', 'top_position', 'rank']\n",
    "        song_name (str): the interested song name\n",
    "    Returns:\n",
    "        the value (str) of the interested attribute of a song on a certain date\n",
    "\n",
    "\n",
    "music_grammy_get_best_artist_by_year(query)\n",
    "    Return the Best New Artist of a certain year in between 1958 and 2019\n",
    "    Args:\n",
    "        query (int, in YYYY format): the interested year\n",
    "    Returns:\n",
    "        the list of artists who win the award\n",
    "\n",
    "\n",
    "music_grammy_get_award_count_by_artist(query)\n",
    "    Return the number of awards won by a certain artist between 1958 and 2019\n",
    "    Args:\n",
    "        query (str): the name of the artist\n",
    "    Returns:\n",
    "        the number of total awards (int)\n",
    "\n",
    "\n",
    "music_grammy_get_award_count_by_song(query)\n",
    "    Return the number of awards won by a song between 1958 and 2019\n",
    "    Args:\n",
    "        query (str): the name of the song\n",
    "    Returns:\n",
    "        the number of total awards (int)\n",
    "\n",
    "\n",
    "music_grammy_get_best_song_by_year(query)\n",
    "    Return the Song Of The Year in a certain year between 1958 and 2019\n",
    "    Args:\n",
    "        query (int, in YYYY format): the interested year\n",
    "    Returns:\n",
    "        the list of the song names that win the Song Of The Year in a certain year\n",
    "\n",
    "\n",
    "music_grammy_get_award_date_by_artist(query)\n",
    "    Return the award winning years of a certain artist\n",
    "    Args:\n",
    "        query (str): the name of the artist\n",
    "    Returns:\n",
    "        the list of years the artist is awarded\n",
    "\n",
    "\n",
    "music_grammy_get_best_album_by_year(query)\n",
    "    Return the Album Of The Year of a certain year between 1958 and 2019\n",
    "    Args:\n",
    "        query (int, in YYYY format): the interested year\n",
    "    Returns:\n",
    "        the list of albums that won the Album Of The Year in a certain year\n",
    "\n",
    "\n",
    "music_grammy_get_all_awarded_artists()\n",
    "    Return all the artists ever awarded Grammy Best New Artist between 1958 and 2019\n",
    "    Returns:\n",
    "        the list of artist ever awarded Grammy Best New Artist (list)\n",
    "\n",
    "\n",
    "music_get_artist_birth_place(query)\n",
    "    Return the birth place country code (2-digit) for the input artist\n",
    "    Args:\n",
    "        query (str): the name of the artist\n",
    "    Returns:\n",
    "        the two-digit country code following ISO-3166 (str)\n",
    "\n",
    "\n",
    "music_get_artist_birth_date(query)\n",
    "    Return the birth date of the artist\n",
    "    Args:\n",
    "        query (str): the name of the artist\n",
    "    Returns:\n",
    "        life_span_begin (str, in YYYY-MM-DD format if possible): the birth date of the person or the begin date of a band\n",
    "\n",
    "\n",
    "music_get_members(query)\n",
    "    Return the member list of a band\n",
    "    Args:\n",
    "        query (str): the name of the band\n",
    "    Returns:\n",
    "        the list of members' names.\n",
    "\n",
    "\n",
    "music_get_lifespan(query)\n",
    "    Return the lifespan of the artist\n",
    "    Args:\n",
    "        query (str): the name of the artist\n",
    "    Returns:\n",
    "        the birth and death dates in a list\n",
    "\n",
    "\n",
    "music_get_song_author(query)\n",
    "    Return the author of the song\n",
    "    Args:\n",
    "        query (str): the name of the song\n",
    "    Returns:\n",
    "        the author of the song (str)\n",
    "\n",
    "\n",
    "music_get_song_release_country(query)\n",
    "    Return the release country of the song\n",
    "    Args:\n",
    "        query (str): the name of the song\n",
    "    Returns:\n",
    "        the two-digit country code following ISO-3166 (str)\n",
    "\n",
    "\n",
    "music_get_song_release_date(query)\n",
    "    Return the release date of the song\n",
    "    Args:\n",
    "        query (str): the name of the song\n",
    "    Returns:\n",
    "        the date of the song (str in YYYY-MM-DD format)\n",
    "\n",
    "\n",
    "music_get_artist_all_works(query)\n",
    "    Return the list of all works of a certain artist\n",
    "    Args:\n",
    "        query (str): the name of the artist\n",
    "    Returns:\n",
    "        the list of all work names\n",
    "\n",
    "\n",
    "sports_soccer_get_games_on_date(date, team_name=None)\n",
    "    Get soccer games given date  \n",
    "    Args:\n",
    "        date (str, in YYYY-MM-DD/YYYY-MM/YYYY format): e.g., 2024-03-01, 2024-03, 2024\n",
    "        team_name (Optional, str)\n",
    "    Returns:\n",
    "        info of the games, such as\n",
    "            venue: whether the team is home or away in game\n",
    "            result: win lose result of the game\n",
    "            GF: goals of the team in game\n",
    "            opponent: opponent of the team\n",
    "            Captain: Captain of the team\n",
    "\n",
    "\n",
    "sports_nba_get_games_on_date(date, team_name=None)\n",
    "    Get all nba game rows given date_str\n",
    "    Args:\n",
    "        date (str, in YYYY-MM-DD/YYYY-MM/YYYY format): the time of the games, e.g. 2023-01-01, 2023-01, 2023\n",
    "        team_name (Optional, str): basketball team name, like Los Angeles Lakers\n",
    "    Returns:\n",
    "        info of the games found, such as\n",
    "            game_id: id of the game\n",
    "            team_name_home: home team name\n",
    "            team_name_away: away team name\n",
    "            wl_home: win lose result of home team\n",
    "            wl_away: win lose result of away team\n",
    "            pts_home: home team points in the game\n",
    "            pts_away: away team points in the game\n",
    "\n",
    "\n",
    "sports_nba_get_play_by_play_data_by_game_ids(query)\n",
    "    Get all nba play by play rows given game ids\n",
    "    Args:\n",
    "        game_ids (List[str]): nba game ids, e.g., [\"0022200547\", \"0029600027\"]\n",
    "    Returns:\n",
    "        info of the play by play events of given game id\n",
    "</DOCUMENTATION>\n",
    "\"\"\".strip()\n",
    "\n",
    "EVAL_SYSTEM_INSTRUCTIONS = \"\"\"\n",
    "You are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You will be given the original question \"ORIGINAL QUESTION\" and answer the \"QUESTION\", which will ask \"Is <answer 1> equivalent to <answer 2>?\". \"Equivalent\" here means that the answers are both suitable in terms of the\n",
    "\n",
    "If the answers are equivalent, you will simply output as \"ANSWER: 1\". If they are not equivalent, you will simply output \"ANSWER: 0\"\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "CONTEXTUAL_EXAMPLES = \"\"\"\n",
    "# Example 1\n",
    "QUESTION: who was the first nba player to get a recorded triple double in basketball?\n",
    "<DOCS>\n",
    "<DOC>\n",
    "Andy Phillip got a triple-double versus the Fort Wayne Pistons on December 14, 1950.\n",
    "</DOC>\n",
    "</DOCS>\n",
    "ANSWER: andy phillip\n",
    "\n",
    "# Example 2\n",
    "QUESTION: what year did ronald reagan become kommissar in the soviet union?\n",
    "<DOCS>\n",
    "<DOC>\n",
    "Ronald Wilson Reagan was an American politician and actor who served as the 40th president of the United States from 1981 to 1989.\n",
    "</DOC>\n",
    "<DOC>\n",
    "Reagan's policies also helped contribute to the end of the Cold War and the end of Soviet communism.\n",
    "</DOC>\n",
    "</DOCS>\n",
    "ANSWER: invalid question\n",
    "\n",
    "# Example 3\n",
    "QUESTION: what are the total number of farms in nebraska?\n",
    "<DOCS>\n",
    "<DOC>\n",
    "There are many farms in Nebraska, some growing corn, while other growing sugar beats.\n",
    "</DOC>\n",
    "<DOC>\n",
    "Nebraska's largest industries are the argiculture and cattle production industries.\n",
    "</DOC>\n",
    "</DOCS>\n",
    "ANSWER: i don't know\n",
    "\"\"\".strip()\n",
    "\n",
    "NO_CONTEXT_CONTEXTUAL_EXAMPLES = \"\"\"\n",
    "# Example 1\n",
    "QUESTION: who was the voice of woody in toy story?\n",
    "ANSWER: Tom Hanks\n",
    "\n",
    "# Example 2\n",
    "QUESTION: what year did ronald reagan become kommissar in the soviet union?\n",
    "ANSWER: Invalid Question\n",
    "\n",
    "# Example 3\n",
    "QUESTION: is brad pitt older than selena gomez?\n",
    "ANSWER: Yes\n",
    "\n",
    "# Example 4\n",
    "QUESTION: what is the age difference between angelina jolie and billy bob thornton?\n",
    "ANSWER: 20 years.\n",
    "\n",
    "# Example 5\n",
    "QUESTION: what is the area of a square that is 10 inches wide and 12 inches tall?\n",
    "ANSWER: 120 inches\n",
    "\n",
    "# Example 6\n",
    "QUESTION: what was the closing price of nvidia yesterday?\n",
    "ANSWER: I don't know\n",
    "\n",
    "# Example 7\n",
    "QUESTION: what is the distance of a marathon race?\n",
    "ANSWER: 26 miles\n",
    "\n",
    "# Example 8\n",
    "QUESTION: if I have $67,312 and I earn 7\\% interest per year compounded yearly, how much will I have after 3 years\n",
    "ANSWER: I don't know\n",
    "\n",
    "# Example 9\n",
    "QUESTION: how many grammy wins does beyoncé have in total?\n",
    "ANSWER: I don't know\n",
    "\n",
    "# Example 10\n",
    "QUESTION: what date was internet explorer released?\n",
    "ANSWER: I don't know\n",
    "\n",
    "# Example 11\n",
    "QUESTION: how far does a bullet travel before it falls onto the ground?\n",
    "ANSWER: I don't know\n",
    "\n",
    "Remember, only answer something other than \"I don't know\" if you are confident you know the correct answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "\n",
    "def prompt_format_v0(prompt, query, query_time, candidates, include_context_examples, tokenizer, token_budget=3500):\n",
    "    bos = \"<|begin_of_text|>\"\n",
    "    sys_message = \"<|start_header_id|>system<|end_header_id|>\"\n",
    "    eos = \"<|eot_id|>\"\n",
    "    user_message = \"<|start_header_id|>user<|end_header_id|>\"\n",
    "    assistant_message = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    segments_text = [f\"<DOC>\\n{segment[0]}\\n</DOC>\" for segment in candidates]\n",
    "    segments_text = '\\n'.join(segments_text).strip()\n",
    "    text_representation = f\"QUESTION (asked at {query_time}): {query}\\n<DOCS>\\n{segments_text}\\n</DOCS>\"\n",
    "    text_representation = tokenizer.decode(tokenizer.encode(text_representation, add_special_tokens=False)[:token_budget])\n",
    "    if include_context_examples:\n",
    "        formatted_text = f\"{bos}{sys_message}\\n{prompt}{eos}{user_message}\\n{CONTEXTUAL_EXAMPLES}\\n{text_representation}{eos}{assistant_message}ANSWER:\"\n",
    "    else:\n",
    "        formatted_text = f\"{bos}{sys_message}\\n{prompt}{eos}{user_message}\\n{text_representation}{eos}{assistant_message}ANSWER:\"\n",
    "    return formatted_text\n",
    "\n",
    "\n",
    "def prompt_format_v1(prompt, query, query_time, candidates, include_context_examples, tokenizer, token_budget=3500):\n",
    "    bos = \"<|begin_of_text|>\"\n",
    "    sys_message = \"<|start_header_id|>system<|end_header_id|>\"\n",
    "    eos = \"<|eot_id|>\"\n",
    "    user_message = \"<|start_header_id|>user<|end_header_id|>\"\n",
    "    assistant_message = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    segments_text = [f\"<DOC>\\n{segment[0]}\\n</DOC>\" for segment in reversed(candidates)]\n",
    "    segments_text = '\\n'.join(segments_text).strip()\n",
    "    text_representation = f\"\\n<DOCS>\\n{segments_text}\\n</DOCS>\\nQUESTION (asked at {query_time}): {query}\"\n",
    "    text_representation = tokenizer.decode(tokenizer.encode(text_representation, add_special_tokens=False)[-token_budget:])\n",
    "    if include_context_examples:\n",
    "        formatted_text = f\"{bos}{sys_message}\\n{prompt}{eos}{user_message}\\n{CONTEXTUAL_EXAMPLES}\\n{text_representation}{eos}{assistant_message}ANSWERABLE:\"\n",
    "    else:\n",
    "        formatted_text = f\"{bos}{sys_message}\\n{prompt}{eos}{user_message}\\n{text_representation}{eos}{assistant_message}ANSWERABLE:\"\n",
    "    return formatted_text\n",
    "\n",
    "\n",
    "def prompt_format_no_context(query, query_time, include_examples, tokenizer, token_budget=3500):\n",
    "    bos = \"<|begin_of_text|>\"\n",
    "    sys_message = \"<|start_header_id|>system<|end_header_id|>\"\n",
    "    eos = \"<|eot_id|>\"\n",
    "    user_message = \"<|start_header_id|>user<|end_header_id|>\"\n",
    "    assistant_message = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    text_representation = f\"\\nQUESTION (asked at {query_time}): {query}\"\n",
    "    text_representation = tokenizer.decode(tokenizer.encode(text_representation, add_special_tokens=False)[-token_budget:])\n",
    "    if include_examples:\n",
    "        formatted_text = f\"{bos}{sys_message}\\n{QA_SYSTEM_INSTRUCTIONS_NO_CONTEXT}{eos}{user_message}\\n{NO_CONTEXT_CONTEXTUAL_EXAMPLES}\\n{text_representation}{eos}{assistant_message}ANSWER:\"\n",
    "    else:\n",
    "        formatted_text = f\"{bos}{sys_message}\\n{QA_SYSTEM_INSTRUCTIONS_NO_CONTEXT}{eos}{user_message}\\n{text_representation}{eos}{assistant_message}ANSWER:\"\n",
    "    return formatted_text\n",
    "\n",
    "\n",
    "class LlamaLLM:\n",
    "    def __init__(self, model_name=MODELS_DIR / \"Meta-Llama-3-8B-Instruct\", peft_path=MODELS_DIR / \"peft_task3\", api_peft_path=MODELS_DIR / \"peft_api\", use_peft=True, qa_prompt=QA_SYSTEM_INSTRUCTIONS, api_prompt=API_SYSTEM_INSTRUCTIONS ,bnb_config=DEFAULT_CONFIG, batch_size=1, prompt_format=\"v1\", include_context_examples=True):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=bnb_config,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        self.use_peft = use_peft\n",
    "        self.peft_model = None\n",
    "        if use_peft:\n",
    "            self.model.load_adapter(peft_path, adapter_name=\"context\")\n",
    "            self.model.load_adapter(api_peft_path, adapter_name=\"api\")\n",
    "        self.qa_prompt = qa_prompt\n",
    "        self.api_prompt = api_prompt\n",
    "        self.batch_size = batch_size\n",
    "        self.model.generation_config.pad_token_ids = self.tokenizer.pad_token_id\n",
    "        self.generation_pipe = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            # max_new_tokens=100,\n",
    "            batch_size=batch_size,\n",
    "            # do_sample=False,\n",
    "            # temperature=0.7,\n",
    "            # top_p=1.0,\n",
    "            truncation=True\n",
    "        )\n",
    "        self.prompt_format = prompt_format\n",
    "        self.include_context_examples = include_context_examples\n",
    "\n",
    "    \"\"\"\n",
    "    Reference prompt structure\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "    {system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "    {prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # def process_example(self, text):\n",
    "    #     with torch.no_grad():\n",
    "    #         bos = \"<|begin_of_text|>\"\n",
    "    #         sys_message = \"<|start_header_id|>system<|end_header_id|>\"\n",
    "    #         eos = \"<|eot_id|>\"\n",
    "    #         user_message = \"<|start_header_id|>user<|end_header_id|>\"\n",
    "    #         assistant_message = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    #         formatted_text = f\"{bos}{sys_message}\\n{self.qa_prompt}{eos}{user_message}\\n{text[:self.max_len]}{eos}{assistant_message}ANSWER:\"\n",
    "    #         result = self.generation_pipe(formatted_text)\n",
    "    #         result = result[0][\"generated_text\"].split(\"ANSWER:\")[-1].strip()\n",
    "    #         return result\n",
    "\n",
    "    # def process_no_candidates(self, query, query_time):\n",
    "    #     formatted_text = prompt_format_no_context(query, query_time, self.include_context_examples, self.tokenizer)\n",
    "    #     result = self.generation_pipe(formatted_text)\n",
    "    #     result = result[0][\"generated_text\"].split(\"ANSWER:\")[-1].strip()\n",
    "    #     return result\n",
    "\n",
    "    def process_candiates(self, query, query_time, candidates):\n",
    "        self.model.enable_adapters()\n",
    "        self.model.set_adapter(\"context\")\n",
    "        with torch.no_grad():\n",
    "            if self.prompt_format == \"v0\":\n",
    "                formatted_text = prompt_format_v0(self.qa_prompt, query, query_time, candidates, self.include_context_examples, self.tokenizer)\n",
    "            elif self.prompt_format == \"v1\":\n",
    "                formatted_text = prompt_format_v1(self.qa_prompt, query, query_time, candidates, self.include_context_examples, self.tokenizer)\n",
    "            else:\n",
    "                formatted_text = prompt_format_no_context(query, query_time, self.include_context_examples, self.tokenizer)\n",
    "            # formatted_text = formatted_text.lower()\n",
    "            result = self.generation_pipe(formatted_text)\n",
    "            # result = result[0][\"generated_text\"].split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].strip()\n",
    "            result = result[0][\"generated_text\"].split(\"ANSWER:\")[-1].strip()\n",
    "            if not result:\n",
    "                result = \"i don't know\"\n",
    "                result = \"NO RESULT\"\n",
    "\n",
    "            # if result == \"i don't know\":\n",
    "            #     return self.process_no_candidates(query, query_time)\n",
    "            return result\n",
    "        ##################\n",
    "        \n",
    "    # def process_candiates(self, query, query_time, candidates):\n",
    "    #     self.model.enable_adapters()\n",
    "    #     self.model.set_adapter(\"context\")\n",
    "    #     with torch.no_grad():\n",
    "    #         if self.prompt_format == \"v0\":\n",
    "    #             formatted_text = prompt_format_v0(self.qa_prompt, query, query_time, candidates, self.include_context_examples, self.tokenizer)\n",
    "    #         elif self.prompt_format == \"v1\":\n",
    "    #             formatted_text = prompt_format_v1(self.qa_prompt, query, query_time, candidates, self.include_context_examples, self.tokenizer)\n",
    "    #         else:\n",
    "    #             formatted_text = prompt_format_no_context(query, query_time, self.include_context_examples, self.tokenizer)\n",
    "    #         # formatted_text = formatted_text.lower()\n",
    "    #         # Create a completion\n",
    "    #         message_text = [{\n",
    "    #             \"role\": \"user\",\n",
    "    #             \"content\": formatted_text\n",
    "    #         }]\n",
    "\n",
    "    #         completion = client.chat.completions.create(\n",
    "    #             model=\"gpt-35-turbo\", # gpt-35-turbo\n",
    "    #             messages=message_text,\n",
    "    #             temperature=0.7,\n",
    "    #             max_tokens=800,\n",
    "    #             top_p=0.95,\n",
    "    #             frequency_penalty=0,\n",
    "    #             presence_penalty=0,\n",
    "    #             stop=None\n",
    "    #         )\n",
    "\n",
    "    #         # print('user:', message_text[0]['content'])\n",
    "    #         # print('assistant:', completion.choices[0].message.content)\n",
    "    #         result = completion.choices[0].message.content\n",
    "    #         # result = result[0][\"generated_text\"].split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].strip()\n",
    "    #         # result = result[0][\"generated_text\"].split(\"ANSWER:\")[-1].strip()\n",
    "    #         if not result:\n",
    "    #             result = \"i don't know\"\n",
    "    #             result = \"NO RESULT\"\n",
    "\n",
    "    #         # if result == \"i don't know\":\n",
    "    #         #     return self.process_no_candidates(query, query_time)\n",
    "    #         return result\n",
    "        ##########################\n",
    "\n",
    "    def process_api(self, text):\n",
    "        self.model.enable_adapters()\n",
    "        self.model.set_adapter(\"api\")\n",
    "        with torch.no_grad():\n",
    "            bos = \"<|begin_of_text|>\"\n",
    "            sys_message = \"<|start_header_id|>system<|end_header_id|>\"\n",
    "            eos = \"<|eot_id|>\"\n",
    "            user_message = \"<|start_header_id|>user<|end_header_id|>\"\n",
    "            assistant_message = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "            formatted_text = f\"{bos}{sys_message}\\n{self.api_prompt}{eos}{user_message}\\n{API_DOCS}\\n{text}{eos}{assistant_message}\"\n",
    "            result = self.generation_pipe(formatted_text)\n",
    "            result = result[0][\"generated_text\"].split(\"<|start_header_id|>assistant<|end_header_id|>\")[1].strip()\n",
    "            return result\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     llm = LlamaLLM(batch_size=4)\n",
    "llm_model = LlamaLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = llm_model.generation_pipe('<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou are a helpful, respectful and honest question answering system. I will be providing you with questions, as well as some additional context that may be helpful in answering those questions. You will be provided with additional information between the \"<DOCS>\" tags. Keep your answers brief, ideally less than 20 words, but a strict limit of 30 words.\\n\\nIf the provided information is insufficient for answering the question, simply output \"Insuffient information\" and only output that. If the question asserts a false premise, like \"When did Eisenhower become Prime Minister?\", simply output \"Invalid question\". Thus, if you are inclined to say something like \"X never has\" or \"X never was\" or \"X never did\", your output should be \"Invalid question\".\\n\\nFinal reminder, with award shows, years can be tricky. Often awards are handed out the year after the project is made. Thus for the Oscars in 2015, the awards are being given out to movies made in 2014.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n# Example 1\\nQUESTION: who was the first nba player to get a recorded triple double in basketball?\\n<DOCS>\\n<DOC>\\nAndy Phillip got a triple-double versus the Fort Wayne Pistons on December 14, 1950.\\n</DOC>\\n</DOCS>\\nANSWER: andy phillip\\n\\n# Example 2\\nQUESTION: what year did ronald reagan become kommissar in the soviet union?\\n<DOCS>\\n<DOC>\\nRonald Wilson Reagan was an American politician and actor who served as the 40th president of the United States from 1981 to 1989.\\n</DOC>\\n<DOC>\\nReagan\\'s policies also helped contribute to the end of the Cold War and the end of Soviet communism.\\n</DOC>\\n</DOCS>\\nANSWER: invalid question\\n\\n# Example 3\\nQUESTION: what are the total number of farms in nebraska?\\n<DOCS>\\n<DOC>\\nThere are many farms in Nebraska, some growing corn, while other growing sugar beats.\\n</DOC>\\n<DOC>\\nNebraska\\'s largest industries are the argiculture and cattle production industries.\\n</DOC>\\n</DOCS>\\nANSWER: i don\\'t know\\n\\n<DOCS>\\n<DOC>\\nMicrosoft Office 2019 - Wikipedia\\nMicrosoft Office 2019 (second release codenamed Office 16) is a version of\\nMicrosoft Office for both Windows and Mac. It replaces Office 2016 and was\\nreplaced by Office 2021 on October 5, 2021.[8] It was unveiled on April 27,\\n2018, for Windows 10 and June 12, 2018, for macOS, and launched on September\\n24, 2018.[1] Some features that had previously been restricted to Office 365\\nsubscribers are available in this release.[9] Office 2019 retains the same\\nmajor version number of 16 that Office 2016 had, making it the second\\nperpetual release of Office 16. Microsoft ended mainstream support for Office\\n2019 on October 10, 2023. Unlike other versions of Microsoft Office, Office\\n2019 will only get two years of extended support, which means that support for\\nOffice 2019 will end on the same day as support for Office 2016 and Windows\\n10, on October 14, 2025.[10]\\n</DOC>\\n<DOC>\\nOffice 2019 vs. Office 2016, Q&A: Licensing, Requirements, Benchmark,...\\nru_project_professional_2019_x86_x64_dvd_d8a4bf9b.iso)\\nFor as long as only one language of Office is installed, it is possible to\\nsetup additional Office applications of the same language, regardless of which\\nlanguage is it.\\n</DOC>\\n<DOC>\\nMicrosoft Office 2019 - Wikipedia\\n## Editions[edit]\\n### Traditional editions[edit]\\nLike its predecessor Microsoft Office 2016, Microsoft Office 2019 has the same\\nperpetual SKU editions aimed towards different markets. Like its predecessor,\\nMicrosoft Office 2019 contains Word, Excel, PowerPoint and OneNote and is\\nlicensed for use on one computer.[18][19]\\n</DOC>\\n<DOC>\\nMicrosoft Office 2019 - Wikipedia\\nToggle the table of contents\\n\\n# Microsoft Office 2019\\n\\n21 languages\\n\\n  * العربية\\n  * বাংলা\\n  * Català\\n  * Čeština\\n  * Deutsch\\n  * Ελληνικά\\n  * Español\\n  * فارسی\\n  * Français\\n  * 한국어\\n  * Bahasa Indonesia\\n  * Italiano\\n  * עברית\\n  * Polski\\n  * Português\\n  * Русский\\n  * Simple English\\n  * Türkçe\\n  * Українська\\n  * Tiếng Việt\\n  * 中文\\n\\nEdit links\\n</DOC>\\n<DOC>\\nMicrosoft Office 2019 - Wikipedia\\n* ^ \"What languages is Office available in?\". Microsoft. Archived from the original on April 7, 2019. Retrieved October 11, 2023.\\n</DOC>\\n<DOC>\\nMicrosoft Office 2019 - Wikipedia\\nFor Office 2013 and 2016, various editions containing the client apps were\\navailable in both Click-To-Run (inspired by Microsoft App-V) and traditional\\nWindows Installer setup formats. However, Office 2019 client apps only have a\\nClick-to-Run installer and only the server apps have the traditional MSI\\ninstaller. The Click-To-Run version has a smaller footprint; in case of\\nMicrosoft Office 2019 Pro Plus, the product requires 10 GB less than the MSI\\nversion of Office 2016 Pro Plus.[22]\\n</DOC>\\n<DOC>\\nMicrosoft Office 2019 - Wikipedia\\nType| Office suite\\nLicense| Trialware, software as a service\\nWebsite| office.com\\nMicrosoft Office 2019 for MacDeveloper(s)| Microsoft  \\n---|---  \\nInitial release| September 24, 2018; 5 years ago (2018-09-24)  \\nOperating system| macOS Sierra or later[6]  \\nPlatform| x64  \\nPredecessor| Microsoft Office 2016  \\nSuccessor| Microsoft Office 2021  \\nAvailable in| 27 languages[7]  \\nList of languagesEnglish, Arabic, Chinese (Simplified), Chinese (Traditional),\\nCzech, Danish, Dutch, Finnish, French, German, Greek, Hebrew, Hungarian,\\nIndonesian, Italian, Japanese, Korean, Norwegian (Bokmål), Polish, Portuguese\\n(Portugal), Portuguese (Brazil), Russian, Slovak, Spanish, Swedish, Thai,\\nTurkish\\n</DOC>\\n<DOC>\\nOffice 2019 vs. Office 2016, Q&A: Licensing, Requirements, Benchmark,...\\nInstalling Office in Russian\\n## Where Can Additional Office Languages Be Found?\\nAdditional languages can be downloaded directly from Microsoft for free from\\nthe following link. Currently Office 2010, 2013, 2016 and 2019 are supported:\\nhttps://go.microsoft.com/fwlink/?LinkId=614981\\n</DOC>\\n<DOC>\\nOffice 2019 vs. Office 2016, Q&A: Licensing, Requirements, Benchmark,...\\nDownloading Additional Languages for Office Directly from Microsoft, for Free\\n## Are Office 2016 & Office 2019 the same? What’s new in Office 2019?\\nThe licensing agreement is more limiting in Office 2019, and it is slower\\ncompared with Office 2016 and 2013. Office 2019 also requires Windows 10 1809,\\nand cannot run on Windows 7, for example.\\n</DOC>\\n<DOC>\\nMicrosoft Office 2019 - Wikipedia\\nPlatform| IA-32, x64, ARM, Web\\nPredecessor| Microsoft Office 2016 (2015)\\nSuccessor| Microsoft Office 2021 (2021)\\nAvailable in| 102 languages[5]\\nList of languages\\n\\n  * Full (40): English, Arabic, Bulgarian, Chinese (Simplified), Chinese (Traditional), Croatian, Czech, Danish, Dutch, Estonian, Finnish, French, German, Greek, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Kazakh, Korean, Latvian, Lithuanian, Malay (Latin), Norwegian Bokmål, Polish, Portuguese (Brazil), Portuguese (Portugal), Romanian, Russian, Serbian (Latin, Serbia), Slovak, Slovenian, Spanish, Swedish, Thai, Turkish, Ukrainian, Vietnamese\\n  * Partial (51): Afrikaans, Albanian, Amharic, Armenian, Assamese, Azerbaijani (Latin), Bangla (Bangladesh), Bangla (Bengali India), Basque (Basque), Belarusian, Bosnian (Latin), Catalan, Dari, Filipino, Galician, Georgian, Gujarati, Icelandic, Irish, Kannada, Khmer, KiSwahili, Konkani, Kyrgyz, Luxembourgish, Macedonian (Republic of Macedonia), Malayalam, Maltese, Maori, Marathi, Mongolian (Cyrillic), Nepali, Norwegian Nynorsk, Odia, Persian (Farsi), Punjabi (Gurmukhi), Quechua, Scottish Gaelic, Serbian (Cyrillic, Bosnia & Herzegovina), Serbian (Cyrillic, Serbia), Sindhi (Arabic), Sinhala, Tamil, Tatar (Cyrillic), Telugu, Turkmen (Latin), Urdu, Uyghur, Uzbek (Latin), Valencian, Welsh\\n  * Proofing only (11): Hausa, Igbo, isiXhosa, isiZulu, Kinyarwanda, Pashto, Romansh, Sesotho sa Leboa, Setswana, Wolof, Yoruba\\n</DOC>\\n</DOCS>\\nQUESTION (asked at 02/28/2024, 10:04:54 PT): is microsoft office 2019 available in a greater number of languages than microsoft office 2013?<|eot_id|><|start_header_id|>assistant<|end_header_id|>ANSWERABLE:')\n",
    "result_ = result[0][\"generated_text\"].split(\"ANSWER:\")[-1].strip()\n",
    "result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mllm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat are the day of week\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m result_ \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANSWER:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      3\u001b[0m result_\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:240\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/transformers/pipelines/base.py:1206\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1199\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1200\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         )\n\u001b[1;32m   1204\u001b[0m     )\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/transformers/pipelines/base.py:1213\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1212\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1213\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/transformers/pipelines/base.py:1112\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1111\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1112\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:327\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/transformers/generation/utils.py:1575\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1568\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1569\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1570\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1572\u001b[0m     )\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1575\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1593\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1594\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1600\u001b[0m     )\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/transformers/generation/utils.py:2697\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2694\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2696\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2697\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2702\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2705\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1196\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1193\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1196\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1209\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1016\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1006\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1007\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         cache_position,\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1016\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:739\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    736\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 739\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    751\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/micromamba/envs/default/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:679\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    670\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[1;32m    671\u001b[0m     query_states,\n\u001b[1;32m    672\u001b[0m     key_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    675\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    676\u001b[0m )\n\u001b[1;32m    678\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 679\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mattn_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m, past_key_value\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = llm_model.generation_pipe(\"what are the day of week\")\n",
    "result_ = result[0][\"generated_text\"].split(\"ANSWER:\")[-1].strip()\n",
    "result_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
