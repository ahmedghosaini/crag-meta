{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73c608a-e0d5-4e0a-b298-3f7539817864",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad7625c-3342-4a33-9f06-60ac741d90a7",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1f5e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdfd4443-b055-455a-9b3c-421292ca63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_small_chunks(documents):\n",
    "    \"\"\"\n",
    "    PyPDFLoader in combination with the recursive textsplitter can lead to very small chunks at the end of a page which only contains a few sentences.\n",
    "    This method merges very small chunks at the end of a page with the previous chunk. Note here documents and chunks are the same thing.\n",
    "    :param documents: all documents/chunks that were returned by the recursive text splitter.\n",
    "    :return: again the list of documents/chunks but with the merged chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remember chunks that have been merged into another chunk to delete them at the end.\n",
    "    deletion_chunks = []\n",
    "\n",
    "    # iterate over all documents/chunks\n",
    "    for i, d in enumerate(documents[1:], start = 1):\n",
    "\n",
    "        # Check if the chunk has less than 50 tokens.\n",
    "        enc = encoding.encode(d.page_content)\n",
    "        if len(enc) < 50:\n",
    "            # If chunk has less than 50 tokens, check if the previous chunk is on the same page.\n",
    "            prev_doc = documents[i-1]\n",
    "            if prev_doc.metadata[\"page\"] == d.metadata[\"page\"]:\n",
    "                # Merge current chunk into the previous chunk.\n",
    "                prev_doc.page_content = combine_strings(prev_doc.page_content, d.page_content)\n",
    "\n",
    "                documents[i-1] = prev_doc\n",
    "\n",
    "                deletion_chunks.append(d)\n",
    "    # Remove merged chunks to prevent duplicates\n",
    "    for i in deletion_chunks:\n",
    "        documents.remove(i)\n",
    "    return documents\n",
    "\n",
    "def combine_strings(s1, s2):\n",
    "    \"\"\"\n",
    "    Combines two strings that have an overlap. It finds the biggest overlap between s1 and s2 and then merges them together.\n",
    "    This method assumes that s1 ends with a substring and that s2 starts with that exact substring.\n",
    "    This method is used to merge two chunks that overlap.\n",
    "    \"\"\"\n",
    "    max_overlap = 0\n",
    "    overlap_index = 0\n",
    "    # This for loops finds the biggest overlap between s1 and s2.\n",
    "    for i in range(len(s1), -1, -1):\n",
    "        if s1[i:] == s2[:len(s1) - i] and len(s1) - i > max_overlap:\n",
    "            max_overlap = len(s1) - i\n",
    "            overlap_index = i\n",
    "    # Concatenate strings considering the biggest overlap between them.\n",
    "    if max_overlap > 0:\n",
    "        return s1[:overlap_index] + \" \" + s2\n",
    "    return s1 + \" \"+ s2\n",
    "\n",
    "def create_df_docs(relevant_docs):\n",
    "    \"\"\"\n",
    "    This method creates a dataframe for the chunks/documents that were used to answer a question.\n",
    "    The resulting dataframe can be displayed in the Chat-interface or added to the Excel sheet for the EGA/upload mode.\n",
    "    :param relevant_docs: chunks that were used as context to answer the questions. It is a list of tuples with the format: (similarity score, Document)\n",
    "    :return: dataframe with columns: Number, Content, Section, Page Number, Relevance, Source\n",
    "    \"\"\"\n",
    "    content = []                # content of the chunk\n",
    "    section = []                # section number or chapter of the chunk\n",
    "    pages = []                  # page number of the chunk\n",
    "    similarities = []           # the similarity score between the question and chunk\n",
    "    files = []                  # file name of the chunk\n",
    "    numbers = []                # index of the chunk\n",
    "\n",
    "    # iterate over all chunks and collect the attributes above.\n",
    "    for i, (doc, sim) in enumerate(relevant_docs):\n",
    "        # similarity treshhold, don't include chunks that exceed it.\n",
    "        if sim >= 0.55:\n",
    "            break\n",
    "        similarities.append(round(1-sim, 2)) # round the similarity score\n",
    "\n",
    "        # To give GPT-4 more guidance and context the chunks contain headers like \"##Part 1.2\" or \"##Header: ... ##Part ...\"\n",
    "        # Here we remove these headers to make it more readable for the user.\n",
    "        #part_number = extract_number(doc.page_content)\n",
    "\n",
    "       \n",
    "        pattern = r'##Part\\s+(\\d+)'\n",
    "        match = re.search(pattern, doc.page_content)\n",
    "        if match:\n",
    "            part_number = int(match.group(1))\n",
    "        else:\n",
    "            part_number = None\n",
    "\n",
    "\n",
    "        if part_number == 1:\n",
    "            pattern = r'##Part[^\\n]*\\n?'\n",
    "        else:\n",
    "            pattern = r'##Header.*?##Part[^\\n]*\\n?'\n",
    "        content.append(re.sub(pattern, '\\n', doc.page_content, flags=re.DOTALL).replace(\"##Header: \", \"\"))\n",
    "        pattern = r'##Header:(.*?)##Part'\n",
    "        section.append(re.findall(pattern, doc.page_content, re.DOTALL))\n",
    "\n",
    "        files.append(doc.metadata[\"source\"].split(\"/\")[-1]) # append file name\n",
    "        pages.append(doc.metadata[\"page\"] +1) # append page number\n",
    "        numbers.append(i+1)\n",
    "\n",
    "    relevant_docs_df = pd.DataFrame({\"Number\":numbers, \"Content\": content, \"Section\":section, \"Page Number\": pages, \"Relevance\": similarities, \"Source\": files})\n",
    "    return relevant_docs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5d02a4-194a-49d1-9b0e-45bee4e22266",
   "metadata": {},
   "source": [
    "Create Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c50bd6a-1ad9-4477-a43d-3f2531f8093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model = model_embedding)\n",
    "all_dbs = {}\n",
    "for file in files:\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(           # Split the Text\n",
    "        model_name=model.split(\".\")[1],\n",
    "        chunk_size=512, \n",
    "        chunk_overlap=128,\n",
    "        separators=[\"\\n\\n\", \". \"],\n",
    "        )\n",
    "    docs = text_splitter.split_documents(documents) \n",
    "    docs = merge_small_chunks(docs)                                                 # Merge the Chunks\n",
    "    all_dbs[file] = FAISS.from_documents(docs, embeddings)                          # Vector Store\n",
    "    all_dbs[\"All PDFs combined\"] = db_combined = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "relevant_docs = all_dbs[\"All PDFs combined\"].similarity_search_with_relevance_scores(query, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e2b796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_values = []\n",
    "for doc in relevant_docs:\n",
    "    sim_values.append(doc[1])\n",
    "# sim_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db7d970a-9917-4e01-af5b-fca1a7f8e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\n",
    "used_pages = set()\n",
    "used_file = set()\n",
    "s = 0\n",
    "number_of_chunks = 0\n",
    "\n",
    "for i, (doc, sim) in enumerate(relevant_docs):\n",
    "    if sim >= 0.55:                                                     # Similarity over 0.55\n",
    "        break\n",
    "    content = doc.page_content\n",
    "    file = doc.metadata[\"source\"].split(\"/\")[-1]\n",
    "    page_number = doc.metadata[\"page\"]\n",
    "\n",
    "    content_tokenized = encoding.encode(content)                        # Tokenize Content\n",
    "    if s + len(content_tokenized) > 5500:\n",
    "        break\n",
    "\n",
    "    s += len(content_tokenized)\n",
    "    context += f\"Extract {i+1}, page: {page_number+1} in {file}:\\n[Start of Extract {i+1}]\\n {content}\\n[End of Extract {i+1}]\\n\\n\"\n",
    "\n",
    "\n",
    "    used_pages.add(page_number+1)\n",
    "    used_file.add(file)\n",
    "    number_of_chunks += 1\n",
    "\n",
    "used_pages = sorted(list(used_pages))\n",
    "used_pages = \", \".join([str(a) for a in used_pages])\n",
    "\n",
    "used_file = sorted(list(used_file))\n",
    "number_of_used_files = len(used_file)\n",
    "used_file = \", \".join([str(a) for a in used_file])\n",
    "\n",
    "\n",
    "prompt = prompt.replace('{##context##}', context)\n",
    "prompt = prompt.replace('{##question##}', query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb8ddc-f9a8-4b6d-ada7-3033615679b9",
   "metadata": {},
   "source": [
    "Sample GPT Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30458a-439b-4252-9606-eb849453ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Request\n",
    "class Answer_Request(BaseModel):\n",
    "    Answer: str | bool | int\n",
    "    Explanation: str\n",
    "    Reference: List[str]\n",
    "\n",
    "class Score_Request(BaseModel):\n",
    "    Score: int\n",
    "    Score_Explanation: str \n",
    "\n",
    "answers = []\n",
    "references = []\n",
    "explanations = []\n",
    "# Create Answers\n",
    "for i in range(5):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        n = 1,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        response_format=Answer_Request, \n",
    "        temperature=temperature,\n",
    "    )\n",
    "    result = completion.choices[0].message.parsed \n",
    "    answers.append(result.Answer)\n",
    "    references.append(result.Reference)\n",
    "    explanations.append(result.Explanation)\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        n = 1,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": metric},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "            These are the Answers: {str(answers)}\n",
    "            This is the Context: {str(context)}\n",
    "            And these are the References {str(references)}\n",
    "            \"\"\"}\n",
    "            ],\n",
    "        response_format=Score_Request,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "score_result= completion.choices[0].message.parsed \n",
    "score_explanation = score_result.Score_Explanation\n",
    "score = score_result.Score\n",
    "print(completion.choices[0].message.parsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
